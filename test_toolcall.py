#!/usr/bin/env python3
"""
OpenRouter + moonshotai/kimi-k2.5 ã®ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆè¤‡é›‘ãªã‚¿ã‚¹ã‚¯ç‰ˆï¼‰
"""
import asyncio
import os
from openai import AsyncOpenAI

# OpenRouterè¨­å®š
client = AsyncOpenAI(
    api_key=os.environ.get("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1"
)

# ã‚ˆã‚Šå¤šãã®ãƒ„ãƒ¼ãƒ«å®šç¾©
tools = [
    {
        "type": "function",
        "function": {
            "name": "read_file",
            "description": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {"type": "string", "description": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"}
                },
                "required": ["path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_dir",
            "description": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã‚’ä¸€è¦§è¡¨ç¤º",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {"type": "string", "description": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹"}
                },
                "required": ["path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "grep",
            "description": "ãƒ•ã‚¡ã‚¤ãƒ«å†…ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢",
            "parameters": {
                "type": "object",
                "properties": {
                    "pattern": {"type": "string", "description": "æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³"},
                    "path": {"type": "string", "description": "æ¤œç´¢å¯¾è±¡ãƒ‘ã‚¹"}
                },
                "required": ["pattern", "path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_file_info",
            "description": "ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—",
            "parameters": {
                "type": "object",
                "properties": {
                    "path": {"type": "string", "description": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"}
                },
                "required": ["path"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "websearch",
            "description": "Webæ¤œç´¢ã‚’å®Ÿè¡Œ",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "æ¤œç´¢ã‚¯ã‚¨ãƒª"}
                },
                "required": ["query"]
            }
        }
    }
]

async def test_complex_parallel(model: str):
    print("=" * 70)
    print(f"Test 1: è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ï¼ˆè¤‡æ•°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª + ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼‰")
    print(f"Model: {model}")
    print("=" * 70)
    
    messages = [
        {"role": "system", "content": """ã‚ãªãŸã¯ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
åŠ¹ç‡çš„ã«ä½œæ¥­ã™ã‚‹ãŸã‚ã€ç‹¬ç«‹ã—ãŸã‚¿ã‚¹ã‚¯ã¯ä¸¦åˆ—ã§ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚"""},
        {"role": "user", "content": """ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ†æã—ã¦ï¼š
1. src, tests, docs ãƒ•ã‚©ãƒ«ãƒ€ã®æ§‹é€ ã‚’ç¢ºèª
2. README.md ã¨ pyproject.toml ã‚’èª­ã‚€
3. "def " ã¨ "class " ãŒã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹æ¤œç´¢"""}
    ]
    
    print(f"\n[User Request]\n{messages[-1]['content']}\n")
    
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        parallel_tool_calls=True,
        temperature=0.7
    )
    
    message = response.choices[0].message
    
    print(f"[Response]")
    print(f"Content: {message.content or '(none)'}")
    print(f"Tool calls: {len(message.tool_calls) if message.tool_calls else 0}")
    
    if message.tool_calls:
        for i, tc in enumerate(message.tool_calls):
            print(f"  [{i+1}] {tc.function.name}: {tc.function.arguments}")
    
    return len(message.tool_calls) if message.tool_calls else 0


async def test_multi_search(model: str):
    print("\n" + "=" * 70)
    print(f"Test 2: è¤‡æ•°ã®æ¤œç´¢ã‚¿ã‚¹ã‚¯")
    print(f"Model: {model}")
    print("=" * 70)
    
    messages = [
        {"role": "system", "content": """ã‚ãªãŸã¯ãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
è¤‡æ•°ã®ç‹¬ç«‹ã—ãŸæ¤œç´¢ã¯åŒæ™‚ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"""},
        {"role": "user", "content": """ä»¥ä¸‹ã«ã¤ã„ã¦èª¿ã¹ã¦ï¼š
1. Python asyncio ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
2. OpenAI API ã®æœ€æ–°æƒ…å ±
3. FastAPI ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
4. pytest ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã®ä½¿ã„æ–¹"""}
    ]
    
    print(f"\n[User Request]\n{messages[-1]['content']}\n")
    
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        parallel_tool_calls=True,
        temperature=0.7
    )
    
    message = response.choices[0].message
    
    print(f"[Response]")
    print(f"Content: {message.content or '(none)'}")
    print(f"Tool calls: {len(message.tool_calls) if message.tool_calls else 0}")
    
    if message.tool_calls:
        for i, tc in enumerate(message.tool_calls):
            print(f"  [{i+1}] {tc.function.name}: {tc.function.arguments}")
    
    return len(message.tool_calls) if message.tool_calls else 0


async def test_file_analysis(model: str):
    print("\n" + "=" * 70)
    print(f"Test 3: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®åŒæ™‚åˆ†æ")
    print(f"Model: {model}")
    print("=" * 70)
    
    messages = [
        {"role": "system", "content": """ã‚ãªãŸã¯ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã§ã™ã€‚
è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚ã«èª­ã‚“ã§åŠ¹ç‡çš„ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚"""},
        {"role": "user", "content": """ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã§æ¯”è¼ƒåˆ†æã—ã¦ï¼š
- src/main.py
- src/utils.py  
- src/config.py
- tests/test_main.py
- tests/test_utils.py"""}
    ]
    
    print(f"\n[User Request]\n{messages[-1]['content']}\n")
    
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        parallel_tool_calls=True,
        temperature=0.7
    )
    
    message = response.choices[0].message
    
    print(f"[Response]")
    print(f"Content: {message.content or '(none)'}")
    print(f"Tool calls: {len(message.tool_calls) if message.tool_calls else 0}")
    
    if message.tool_calls:
        for i, tc in enumerate(message.tool_calls):
            print(f"  [{i+1}] {tc.function.name}: {tc.function.arguments}")
    
    return len(message.tool_calls) if message.tool_calls else 0


async def test_tool_result_handling(model: str):
    """ãƒ„ãƒ¼ãƒ«çµæœã‚’è¿”ã—ãŸå¾Œã®ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 70)
    print(f"Test: Tool Result Handling (Full Flow)")
    print(f"Model: {model}")
    print("=" * 70)
    
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": "srcãƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’è¦‹ã›ã¦"}
    ]
    
    print(f"\n[Step 1] Initial request")
    
    # Step 1: Get tool call
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        parallel_tool_calls=True,
        temperature=0.7
    )
    
    message = response.choices[0].message
    print(f"  Tool calls: {len(message.tool_calls) if message.tool_calls else 0}")
    
    # Debug: check all attributes of message
    print(f"  [DEBUG] Message attributes: {[a for a in dir(message) if not a.startswith('_')]}")
    if hasattr(message, 'reasoning_content'):
        print(f"  [DEBUG] reasoning_content: {message.reasoning_content[:100] if message.reasoning_content else 'None'}...")
    if hasattr(message, 'reasoning'):
        print(f"  [DEBUG] reasoning: {message.reasoning[:100] if message.reasoning else 'None'}...")
    
    if not message.tool_calls:
        print("  No tool calls, model responded directly")
        print(f"  Content: {message.content}")
        return
    
    # Add assistant message with tool calls (including reasoning_content if present)
    assistant_msg = {
        "role": "assistant",
        "content": message.content or "",
        "tool_calls": [
            {
                "id": tc.id,
                "type": "function",
                "function": {
                    "name": tc.function.name,
                    "arguments": tc.function.arguments
                }
            }
            for tc in message.tool_calls
        ]
    }
    # Include reasoning_content for models like kimi-k2.5
    # Note: OpenRouter returns 'reasoning' but expects 'reasoning_content' in the request
    if hasattr(message, 'reasoning') and message.reasoning:
        assistant_msg["reasoning_content"] = message.reasoning
        print(f"  [Has reasoning: {len(message.reasoning)} chars]")
    elif hasattr(message, 'reasoning_content') and message.reasoning_content:
        assistant_msg["reasoning_content"] = message.reasoning_content
        print(f"  [Has reasoning_content: {len(message.reasoning_content)} chars]")
    messages.append(assistant_msg)
    
    # Add tool results
    for tc in message.tool_calls:
        print(f"  [{tc.function.name}] Simulating tool result...")
        messages.append({
            "role": "tool",
            "tool_call_id": tc.id,
            "content": "main.py\nutils.py\nconfig.py\n__init__.py"
        })
    
    print(f"\n[Step 2] Sending tool results back to model...")
    
    # Step 2: Send tool results and get final response
    try:
        response2 = await client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            parallel_tool_calls=True,
            temperature=0.7
        )
        
        message2 = response2.choices[0].message
        print(f"  Content: {message2.content[:200] if message2.content else '(none)'}...")
        print(f"  More tool calls: {len(message2.tool_calls) if message2.tool_calls else 0}")
        print("\nâœ… Model handled tool results correctly!")
        
    except Exception as e:
        print(f"\nâŒ Error after sending tool results: {e}")


async def test_streaming_tool_result(model: str):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ã®ãƒ„ãƒ¼ãƒ«çµæœå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 70)
    print(f"Test: Streaming After Tool Results")
    print(f"Model: {model}")
    print("=" * 70)
    
    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": "ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•™ãˆã¦"}
    ]
    
    # Step 1: Initial streaming request
    print(f"\n[Step 1] Streaming initial request...")
    
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        parallel_tool_calls=True,
        temperature=0.7,
        stream=True
    )
    
    collected_content = ""
    collected_reasoning = ""
    tool_calls_dict = {}
    
    async for chunk in response:
        delta = chunk.choices[0].delta if chunk.choices else None
        if not delta:
            continue
        
        if delta.content:
            collected_content += delta.content
        
        # Collect reasoning from streaming chunks
        if hasattr(delta, 'reasoning') and delta.reasoning:
            collected_reasoning += delta.reasoning
        
        if delta.tool_calls:
            for tc in delta.tool_calls:
                idx = tc.index
                if idx not in tool_calls_dict:
                    tool_calls_dict[idx] = {"id": "", "name": "", "arguments": ""}
                if tc.id:
                    tool_calls_dict[idx]["id"] = tc.id
                if tc.function:
                    if tc.function.name:
                        tool_calls_dict[idx]["name"] = tc.function.name
                    if tc.function.arguments:
                        tool_calls_dict[idx]["arguments"] += tc.function.arguments
    
    print(f"  Content: {collected_content[:100] if collected_content else '(none)'}")
    print(f"  Reasoning: {len(collected_reasoning)} chars")
    print(f"  Tool calls: {len(tool_calls_dict)}")
    
    if not tool_calls_dict:
        print("  No tool calls in streaming")
        return
    
    for idx, tc in tool_calls_dict.items():
        print(f"    [{idx}] {tc['name']}: {tc['arguments'][:50]}...")
    
    # Build messages with tool results (including reasoning_content for kimi-k2.5)
    assistant_msg = {
        "role": "assistant",
        "content": collected_content,
        "tool_calls": [
            {
                "id": tc["id"] or f"call_{idx}",
                "type": "function",
                "function": {"name": tc["name"], "arguments": tc["arguments"]}
            }
            for idx, tc in tool_calls_dict.items()
        ]
    }
    if collected_reasoning:
        assistant_msg["reasoning_content"] = collected_reasoning
    messages.append(assistant_msg)
    
    for idx, tc in tool_calls_dict.items():
        messages.append({
            "role": "tool",
            "tool_call_id": tc["id"] or f"call_{idx}",
            "content": "Mock search result: Today's news includes technology updates and sports news."
        })
    
    print(f"\n[Step 2] Streaming after tool results...")
    
    try:
        response2 = await client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            parallel_tool_calls=True,
            temperature=0.7,
            stream=True
        )
        
        final_content = ""
        async for chunk in response2:
            delta = chunk.choices[0].delta if chunk.choices else None
            if delta and delta.content:
                final_content += delta.content
                print(delta.content, end="", flush=True)
        
        print(f"\n\nâœ… Streaming completed! ({len(final_content)} chars)")
        
    except Exception as e:
        print(f"\nâŒ Streaming error: {e}")


if __name__ == "__main__":
    model = os.environ.get("TEST_MODEL", "moonshotai/kimi-k2.5")
    
    print("\n" + "ğŸ”§" * 35)
    print(f"  OpenRouter Tool Result Handling Test")
    print(f"  Model: {model}")
    print("ğŸ”§" * 35)
    
    # ãƒ„ãƒ¼ãƒ«çµæœå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
    asyncio.run(test_tool_result_handling(model))
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ã®ãƒ„ãƒ¼ãƒ«çµæœå‡¦ç†ãƒ†ã‚¹ãƒˆ
    asyncio.run(test_streaming_tool_result(model))
