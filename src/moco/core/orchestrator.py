import os
import re
import json
import asyncio
import time
import logging
from pathlib import Path
from typing import Dict, Optional, List, Callable, Any
from ..tools.discovery import AgentLoader, AgentConfig, discover_tools
from ..tools.skill_loader import SkillLoader, SkillConfig
from ..tools.skill_tools import get_loaded_skills, clear_session_skills
from ..cancellation import check_cancelled, clear_cancel_event, OperationCancelled
from .runtime import AgentRuntime, LLMProvider
from ..storage.session_logger import SessionLogger
from ..storage.semantic_memory import SemanticMemory

# Optimizer components
from .optimizer import (
    TaskAnalyzer,
    AgentSelector,
    QualityTracker,
    QualityEvaluator,
    OptimizerConfig,
    ExecutionMetrics,
    SelectionResult
)
from ..utils.json_parser import SmartJSONParser

try:
    from rich.console import Console
    from rich.spinner import Spinner
    from rich.live import Live
    _console = Console()

    def _log_delegation(agent_name: str, task_summary: str = ""):
        # ã‚¿ã‚¹ã‚¯ã®æ¦‚è¦ã‚’çŸ­ãåˆ‡ã‚Šè©°ã‚
        summary = task_summary[:60].replace('\n', ' ').strip()
        if len(task_summary) > 60:
            summary += "..."
        _console.print(f"  [dim]â†’[/dim] [cyan]@{agent_name}[/cyan] [dim]{summary}[/dim]")
except ImportError:
    _console = None
    def _log_delegation(agent_name: str, task_summary: str = ""):
        summary = task_summary[:60].replace('\n', ' ').strip()
        if len(task_summary) > 60:
            summary += "..."
        print(f"  â†’ @{agent_name} {summary}")


def moco_log(message: str, verbose: bool = False):
    """
    è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›ã€‚
    UIãŒåˆ©ç”¨å¯èƒ½ãªã‚‰UIã®è©³ç´°ãƒ­ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«è¡¨ç¤ºã€
    ãã†ã§ãªã‘ã‚Œã°ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«dimè¡¨ç¤ºã€‚
    """
    if not verbose:
        return
    
    clean_msg = str(message).strip().replace('\n', ' ')
    if len(clean_msg) > 100:
        clean_msg = clean_msg[:97] + "..."

    try:
        from ..ui.layout import ui_state
        ui_state.add_verbose_log(clean_msg)
    except (ImportError, Exception):
        # UIæœªä½¿ç”¨æ™‚ã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        if _console:
            _console.print(f"[dim]{clean_msg}[/dim]")
        else:
            print(clean_msg)


class Orchestrator:
    """
    ãƒ¡ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã€
    ä¼šè©±å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ã€‚

    å„ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç‹¬ç«‹ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ã‚’æŒã¤ã€‚
    """

    def __init__(
        self,
        profile: str = 'default',
        session_logger: Optional[SessionLogger] = None,
        provider: str = None,
        model: str = None,
        stream: bool = True,
        verbose: bool = False,
        progress_callback: Optional[callable] = None,
        use_optimizer: bool = False,
        working_directory: Optional[str] = None
    ):
        self.profile = profile
        self.provider = provider  # None = ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ±ºå®š
        self.model = model        # None = ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        self.stream = stream      # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
        self.verbose = verbose    # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
        self.progress_callback = progress_callback  # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.use_optimizer = use_optimizer  # Optimizerä½¿ç”¨ãƒ•ãƒ©ã‚°
        self.working_directory = working_directory or os.getcwd()  # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.name = "orchestrator"
        self.loader = AgentLoader(profile=self.profile)
        self.skill_loader = SkillLoader(profile=self.profile)
        self._all_skills: Dict[str, SkillConfig] = {}  # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ã‚­ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._session_skills: List[SkillConfig] = []   # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚¹ã‚­ãƒ«ï¼ˆãƒ—ãƒ¼ãƒ«ï¼‰
        
        # Optimizer ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.optimizer_config = OptimizerConfig(profile=profile)
        self.task_analyzer = TaskAnalyzer(
            llm_generate_fn=self._llm_generate_for_optimizer
        )
        self.agent_selector = AgentSelector(self.optimizer_config)
        self.quality_tracker = QualityTracker()
        self.quality_evaluator = QualityEvaluator(
            llm_generate_fn=self._llm_generate_for_optimizer
        )
        
        # å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”¨
        self._current_execution_start: Optional[float] = None
        self._current_scores: Optional[dict] = None
        self._current_selection: Optional[SelectionResult] = None
        self._inline_evaluations: Dict[str, dict] = {}  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå -> è©•ä¾¡çµæœ
        self._agent_execution_metrics: List[Any] = []   # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¡ãƒ¢ãƒªã®åˆæœŸåŒ–
        db_path = os.getenv("SEMANTIC_DB_PATH", str(Path.cwd() / "data" / "semantic.db"))
        self.semantic_memory = SemanticMemory(db_path=db_path)

        # ãƒ„ãƒ¼ãƒ«ã‚’å‹•çš„ã«èª­ã¿è¾¼ã‚€
        self.tool_map = discover_tools(profile=self.profile)

        self.agents: Dict[str, AgentConfig] = {}
        self.runtimes: Dict[str, AgentRuntime] = {}

        # ä¼šè©±å±¥æ­´ç®¡ç†
        self.session_logger = session_logger or SessionLogger()
        self._current_session_id: Optional[str] = None

        # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        # {parent_session_id: {agent_name: sub_session_id}}
        self._sub_sessions: Dict[str, Dict[str, str]] = {}

        self.reload_agents()

    def reload_agents(self):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®šç¾©ã‚’å†èª­ã¿è¾¼ã¿ã™ã‚‹"""
        self.agents = self.loader.load_agents()
        self.runtimes = {}
        
        # Skills ã‚’ãƒ­ãƒ¼ãƒ‰
        self._all_skills = self.skill_loader.load_skills()
        if self.verbose and self._all_skills:
            print(f"[Skills] Loaded {len(self._all_skills)} skills: {list(self._all_skills.keys())}")

        # åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸€è¦§ï¼ˆorchestrator ä»¥å¤–ï¼‰
        available_agents = [name for name in self.agents.keys() if name != "orchestrator"]

        for name, config in self.agents.items():
            if self.verbose:
                print(f"Loaded agent: {name}")

            # delegate_to_agent ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®š
            needs_delegate = name == "orchestrator" or "delegate_to_agent" in config.tools
            
            if needs_delegate:
                # delegate_to_agent ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
                agent_tools = dict(self.tool_map)
                agent_tools["delegate_to_agent"] = self._create_delegate_tool(available_agents)
                self.runtimes[name] = AgentRuntime(
                    config,
                    agent_tools,
                    agent_name=name,
                    name=name,
                    provider=self.provider,
                    model=self.model,
                    stream=self.stream,
                    verbose=self.verbose,
                    progress_callback=self.progress_callback,
                    parent_agent=None if name == "orchestrator" else "orchestrator",
                    semantic_memory=self.semantic_memory
                )
            else:
                self.runtimes[name] = AgentRuntime(
                    config,
                    self.tool_map,
                    agent_name=name,
                    name=name,
                    provider=self.provider,
                    model=self.model,
                    stream=self.stream,
                    verbose=self.verbose,
                    progress_callback=self.progress_callback,
                    parent_agent="orchestrator",  # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦ªã¯orchestrator
                    semantic_memory=self.semantic_memory
                )

    def _create_delegate_tool(self, available_agents: list):
        """delegate_to_agent ãƒ„ãƒ¼ãƒ«ã‚’å‹•çš„ã«ä½œæˆ"""
        orchestrator = self

        def delegate_to_agent(agent_name: str, task: str) -> str:
            """
            ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚¿ã‚¹ã‚¯ã‚’å§”è­²ã—ã¾ã™ã€‚

            Args:
                agent_name: å§”è­²å…ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåï¼ˆä¾‹: backend-coder, code-reviewerï¼‰
                task: ã‚¿ã‚¹ã‚¯ã®è©³ç´°ãªèª¬æ˜

            Returns:
                ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœ
            """
            if agent_name not in available_agents:
                return f"Error: Unknown agent '{agent_name}'. Available: {', '.join(available_agents)}"

            _log_delegation(agent_name, task)

            # å…±é€šã®å§”è­²ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆå±¥æ­´ç®¡ç†ã‚„ãƒ­ã‚°è¨˜éŒ²ã‚’å«ã‚€ï¼‰
            return orchestrator._delegate_to_agent(
                agent_name, 
                task, 
                orchestrator._current_session_id
            )

        return delegate_to_agent

    def _get_or_create_sub_session(
        self,
        parent_session_id: str,
        agent_name: str
    ) -> str:
        """
        ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—ã¾ãŸã¯ä½œæˆã™ã‚‹ã€‚
        åŒã˜è¦ªã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ã¯åŒã˜ã‚µãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å†åˆ©ç”¨ã™ã‚‹ã€‚
        """
        if parent_session_id not in self._sub_sessions:
            self._sub_sessions[parent_session_id] = {}

        if agent_name not in self._sub_sessions[parent_session_id]:
            # æ–°ã—ã„ã‚µãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            sub_session_id = self.session_logger.create_session(
                title=f"Sub: @{agent_name}",
                parent_session_id=parent_session_id,
                agent_name=agent_name
            )
            self._sub_sessions[parent_session_id][agent_name] = sub_session_id

        return self._sub_sessions[parent_session_id][agent_name]

    def _prepare_session(self, user_input: str, session_id: Optional[str] = None) -> tuple[str, List[Any]]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æº–å‚™ã¨å±¥æ­´ã®å–å¾—ã‚’è¡Œã†"""
        if not session_id:
            session_id = self.create_session(title=user_input[:50])

        # å±¥æ­´ã‚’å–å¾—ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        history_format = "openai" if self.provider in (LLMProvider.OPENAI, LLMProvider.OPENROUTER) else "gemini"
        history = self.session_logger.get_agent_history(session_id, format=history_format)
        
        return session_id, history

    async def process_message(
        self,
        user_input: str,
        session_id: Optional[str] = None,
        history: Optional[List[Any]] = None
    ) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†ã—ã€é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹

        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆå±¥æ­´ç®¡ç†ç”¨ï¼‰
            history: äº‹å‰ã«å–å¾—ã—ãŸå±¥æ­´ï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•å–å¾—ï¼‰
        """
        # å®Ÿè¡Œé–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        self._current_execution_start = time.time()
        
        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        self._inline_evaluations = {}
        self._agent_execution_metrics = []
        
        # ã‚¹ã‚­ãƒ«ãƒ—ãƒ¼ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ–°ã—ã„ã‚¿ã‚¹ã‚¯ã”ã¨ã«ã‚¯ãƒªã‚¢ï¼‰
        self._session_skills = []
        clear_session_skills()  # skill_tools ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ã‚¯ãƒªã‚¢
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ä¿æŒ
        self._current_session_id = session_id

        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
        if session_id:
            check_cancelled(session_id)

        # Todoãƒ„ãƒ¼ãƒ«ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®š
        if session_id:
            from ..tools.todo import set_current_session as set_todo_session
            from ..tools.stats import set_current_session as set_stats_session
            set_todo_session(session_id)
            set_stats_session(session_id)
        
        # Optimizer: ã‚¿ã‚¹ã‚¯åˆ†æï¼ˆuse_optimizer=True ã®å ´åˆã®ã¿ï¼‰
        if self.use_optimizer:
            self._current_scores = await self.task_analyzer.analyze(user_input)
            available_agents = [name for name in self.agents.keys()]
            self._current_selection = self.agent_selector.select(
                self._current_scores, 
                available_agents
            )
            
            moco_log(f"[Optimizer] Scores: {self._current_scores}", self.verbose)
            moco_log(f"[Optimizer] Selection: {self._current_selection.depth} -> {self._current_selection.agents}", self.verbose)
        else:
            # Optimizerç„¡åŠ¹æ™‚ã¯å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½¿ç”¨
            self._current_scores = None
            self._current_selection = SelectionResult(
                depth="full",
                agents=[name for name in self.agents.keys() if name != "orchestrator"],
                reason="Optimizer disabled"
            )

        # Orchestratorã®å±¥æ­´ã‚’å–å¾—ï¼ˆæ¸¡ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        if history is None and session_id:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å±¥æ­´ã‚’å–å¾—
            history_format = "openai" if self.provider in (LLMProvider.OPENAI, LLMProvider.OPENROUTER) else "gemini"
            history = self.session_logger.get_agent_history(
                session_id,
                limit=20,
                format=history_format
            )

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²ï¼ˆOrchestratorã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ï¼‰
        if session_id:
            self.session_logger.log_agent_message(session_id, "user", user_input)

        # @agent-name ã®æ¤œå‡ºï¼ˆãƒã‚¤ãƒ•ãƒ³å«ã‚€åå‰ã«å¯¾å¿œï¼‰
        match = re.match(r"^@([\w-]+)[:\s]*(.*)", user_input, re.DOTALL)
        
        response = ""
        current_agent = "orchestrator"
        try:
            if match:
                agent_name = match.group(1)
                current_agent = agent_name
                query = match.group(2).lstrip(': ').strip()

                if agent_name in self.runtimes:
                    # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                    if session_id:
                        check_cancelled(session_id)
                    # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç‹¬ç«‹ã—ãŸå±¥æ­´ã‚’ä½¿ç”¨
                    response = await self._delegate_to_agent(agent_name, query, session_id)
                    # å®Ÿè¡Œå¾Œã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                    if session_id:
                        check_cancelled(session_id)
                else:
                    response = f"Error: Agent '@{agent_name}' not found. Available agents: {list(self.agents.keys())}"
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æŒ™å‹•ï¼šorchestratorã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ä»»ã›ã‚‹
                if "orchestrator" in self.runtimes:
                    # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                    if session_id:
                        check_cancelled(session_id)
                    # orchestratorã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¯Orchestratorã®å±¥æ­´ã‚’æ¸¡ã™
                    response = await self._delegate_to_orchestrator_agent(user_input, session_id, history)
                    # å®Ÿè¡Œå¾Œã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                    if session_id:
                        check_cancelled(session_id)
                else:
                    response = self._list_agents()
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã€éƒ¨åˆ†å¿œç­”ã‚’ä¿å­˜
            if session_id and current_agent in self.runtimes:
                partial = self.runtimes[current_agent]._partial_response
                if partial:
                    self.session_logger.log_agent_message(
                        session_id, 
                        "assistant", 
                        f"[ã‚¨ãƒ©ãƒ¼ã§ä¸­æ–­: {type(e).__name__}]\n{partial}"
                    )
            raise

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’è¨˜éŒ²ï¼ˆOrchestratorã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ï¼‰
        if session_id:
            self.session_logger.log_agent_message(session_id, "assistant", response)
        
        # Optimizer: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        await self._record_execution_metrics(session_id, user_input, response)

        return response
    
    async def _record_execution_metrics(
        self,
        session_id: Optional[str],
        user_input: str,
        response: str
    ) -> None:
        """å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        if not self._current_scores or not self._current_selection:
            return
        
        try:
            duration = time.time() - (self._current_execution_start or time.time())
            
            # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—
            total_tokens = 0
            # Orchestratorè‡ªèº«ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‹ã‚‰å–å¾—
            if "orchestrator" in self.runtimes:
                metrics = self.runtimes["orchestrator"].get_metrics()
                total_tokens += metrics.get("total_tokens", 0)

            # å§”è­²å…ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆ†ã‚‚åˆç®—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            for name in self._current_selection.agents:
                if name in self.runtimes:
                    metrics = self.runtimes[name].get_metrics()
                    total_tokens += metrics.get("total_tokens", 0)

            # å“è³ªè©•ä¾¡ï¼šã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡å„ªå…ˆã€ãªã‘ã‚Œã°LLMè©•ä¾¡
            aggregated_eval = self._aggregate_inline_evaluations()
            if aggregated_eval:
                # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’ä½¿ç”¨ï¼ˆè¿½åŠ LLMã‚³ãƒ¼ãƒ«ãªã—ï¼‰
                quality_scores = {
                    "completion": int(aggregated_eval.get("completion", 5)),
                    "quality": int(aggregated_eval.get("quality", 5)),
                    "task_complexity": int(aggregated_eval.get("task_complexity", 5)),
                    "prompt_specificity": int(aggregated_eval.get("prompt_specificity", 5)),
                }
                if self.verbose:
                    print(f"[Optimizer] Using inline evaluation from {len(self._inline_evaluations)} agents")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šQualityEvaluatorã‚’ä½¿ç”¨
                quality_scores = await self.quality_evaluator.evaluate(
                    task=user_input,
                    result=response,
                    profile=self.profile
                )
            ai_score = (quality_scores.get("completion", 5) + quality_scores.get("quality", 5)) / 20.0

            # å®Ÿè¡Œä¸­ã«è¨ˆæ¸¬ã—ãŸæŒ‡æ¨™
            delegation_count = len(self._current_selection.agents)
            todo_used = 1 if "todowrite" in response.lower() or "todo" in response.lower() else 0
            input_length = len(user_input)
            output_length = len(response)
            
            # å±¥æ­´ãƒ»è¦ç´„é–¢é€£ã®æŒ‡æ¨™
            history_turns = 0
            summary_depth = 0
            if session_id and self.session_logger:
                # å±¥æ­´ã®ã‚¿ãƒ¼ãƒ³æ•°ã‚’å–å¾—
                history = self.session_logger.get_agent_history(session_id, limit=100)
                history_turns = len(history) if history else 0
                # è¦ç´„ã®æ·±ã•ã‚’å–å¾—
                summary_depth = self.session_logger.get_summary_depth(session_id)

            execution = ExecutionMetrics(
                tokens=total_tokens,
                duration=duration,
                tool_calls=response.count("@"),  # å§”è­²å›æ•°ã®æ¦‚ç®—
                errors=1 if "Error" in response else 0,
                retries=0,
                has_apology="ç”³ã—è¨³" in response or "sorry" in response.lower()
            )
            
            record_id = self.quality_tracker.record(
                profile=self.profile,
                session_id=session_id or "unknown",
                task_summary=user_input[:100],
                scores=self._current_scores,
                selection=self._current_selection,
                execution=execution,
                thresholds=self.optimizer_config.get_thresholds(),
                ai_score=ai_score,
                task_complexity=quality_scores.get("task_complexity", 5),
                prompt_specificity=quality_scores.get("prompt_specificity", 5),
                todo_used=todo_used,
                delegation_count=delegation_count,
                input_length=input_length,
                output_length=output_length,
                history_turns=history_turns,
                summary_depth=summary_depth
            )
            
            # orchestrator è‡ªèº«ã®å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚ agent_executions ã«è¨˜éŒ²ã™ã‚‹
            # ï¼ˆInsight Panel ã® Agent Stats ã« orchestrator ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ï¼‰
            try:
                from moco.core.optimizer import AgentExecutionMetrics

                # æ—¢ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã¯é‡è¤‡ã‚’é¿ã‘ã‚‹
                if self._agent_execution_metrics:
                    self._agent_execution_metrics = [
                        m for m in self._agent_execution_metrics if getattr(m, "agent_name", None) != "orchestrator"
                    ]

                o_prompt_tokens = 0
                o_completion_tokens = 0
                try:
                    o_runtime = self.runtimes.get("orchestrator")
                    if o_runtime:
                        o_usage = o_runtime.get_metrics() or {}
                        o_prompt_tokens = int(o_usage.get("prompt_tokens", 0) or 0)
                        o_completion_tokens = int(o_usage.get("completion_tokens", 0) or 0)
                except Exception:
                    pass  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å¤±æ•—ã¯ç„¡è¦–

                orch_metric = AgentExecutionMetrics(
                    agent_name="orchestrator",
                    parent_agent=None,
                    tokens_input=o_prompt_tokens,
                    tokens_output=o_completion_tokens,
                    execution_time_ms=int(duration * 1000),
                    tool_calls=0,
                    inline_score=float(ai_score),
                    eval_completion=None,
                    eval_quality=None,
                    eval_task_complexity=None,
                    eval_prompt_specificity=None,
                    summary_depth=int(summary_depth or 0),
                    history_turns=int(history_turns or 0),
                    error_message=None,
                )

                if self._agent_execution_metrics is None:
                    self._agent_execution_metrics = []
                self._agent_execution_metrics.insert(0, orch_metric)
            except Exception:
                pass

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
            if self._agent_execution_metrics:
                self.quality_tracker.record_agent_executions(
                    request_id=record_id,
                    agents=self._agent_execution_metrics
                )
                if self.verbose:
                    print(f"[Optimizer] Recorded {len(self._agent_execution_metrics)} agent executions")
                    
        except Exception as e:
            if self.verbose:
                print(f"[Optimizer] Failed to record metrics: {e}")

    async def _delegate_to_orchestrator_agent(
        self,
        query: str,
        session_id: Optional[str],
        history: Optional[List[Any]]
    ) -> str:
        """orchestratorã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å§”è­²ï¼ˆOrchestratorã®å±¥æ­´ã‚’ä½¿ç”¨ï¼‰

        Note: orchestrator ã¯ delegate_to_agent ãƒ„ãƒ¼ãƒ«ã‚’æŒã£ã¦ãŠã‚Šã€
        ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®å§”è­²ã¯ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã§è¡Œã‚ã‚Œã‚‹ã€‚
        ãƒ„ãƒ¼ãƒ«çµæœã‚’å—ã‘å–ã£ãŸå¾Œã€orchestrator ãŒæœ€çµ‚å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        runtime = self.runtimes["orchestrator"]
        
        # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæƒ…å ±ã‚’è‡ªå‹•æ³¨å…¥
        working_dir_prefix = self._get_working_context_prompt()
        query_with_workdir = working_dir_prefix + query

        # Optimizer ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æ³¨å…¥ï¼ˆuse_optimizer=True ã‹ã¤é¸æŠçµæœãŒã‚ã‚‹å ´åˆï¼‰
        if self.use_optimizer and self._current_selection:
            optimizer_guidance = f'''ã€Optimizer ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€‘
æ·±åº¦: {self._current_selection.depth}
æ¨å¥¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {', '.join(self._current_selection.agents) if self._current_selection.agents else 'ãªã—'}
ã‚¹ã‚­ãƒƒãƒ—æ¨å¥¨: {', '.join(self._current_selection.skipped) if self._current_selection.skipped else 'ãªã—'}
ç†ç”±: {self._current_selection.reason}

â€» ä¸Šè¨˜ã¯æ¨å¥¨ã§ã™ã€‚ã‚¿ã‚¹ã‚¯ã®æ€§è³ªä¸Šã€è¿½åŠ ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¿…è¦ãªå ´åˆã¯åˆ¤æ–­ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

'''
            query_with_workdir = optimizer_guidance + query_with_workdir
            
            if self.verbose:
                print(f'[Optimizer] Injecting guidance: depth={self._current_selection.depth}, agents={self._current_selection.agents}')

        try:
            # AgentRuntime.run ã¯ async
            response = await runtime.run(query_with_workdir, history=history, session_id=session_id)
            # å¿œç­”å†…ã® @agent-name æŒ‡ç¤ºã‚’è‡ªå‹•å‡¦ç†ã™ã‚‹
            response = await self._process_delegations_in_response(response, session_id)

            # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡çµæœã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«è¿½åŠ 
            if self._inline_evaluations:
                eval_summary = self._aggregate_inline_evaluations()
                eval_block = f"""
---
ã€ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã€‘
- completion: {int(eval_summary.get('completion', 5))}
- quality: {int(eval_summary.get('quality', 5))}
- task_complexity: {int(eval_summary.get('task_complexity', 5))}
- prompt_specificity: {int(eval_summary.get('prompt_specificity', 5))}
"""
                response = response + eval_block

            return f"@orchestrator: {response}"
        except Exception as e:
            return f"Error running @orchestrator: {e}"

    async def _process_delegations_in_response(
        self,
        response: str,
        session_id: Optional[str]
    ) -> str:
        """
        ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å¿œç­”ã«å«ã¾ã‚Œã‚‹ @agent-name ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€
        è©²å½“ã™ã‚‹ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å‡¦ç†ã‚’å§”è­²ã™ã‚‹ã€‚

        å§”è­²ã‚¿ã‚¹ã‚¯ã¯ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
        """
        # responseãŒNoneã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        if response is None:
            return ""

        # å¿œç­”å†…ã® @agent-name ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        # ä¾‹: "@doc-writer ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„..."
        lines = response.split('\n')
        
        # ã¾ãšå…¨ã¦ã®å§”è­²ã‚¿ã‚¹ã‚¯ã‚’æ¤œå‡ºï¼ˆç¬¬1ãƒ‘ã‚¹ï¼‰
        delegations = []  # [(agent_name, instruction, start_line_index, end_line_index)]
        i = 0

        while i < len(lines):
            # ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
            if session_id:
                check_cancelled(session_id)

            line = lines[i]
            # ã‚³ãƒ­ãƒ³ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã«æŸ”è»Ÿã«å¯¾å¿œ
            match = re.match(r"^@([\w-]+)[:\s]*(.*)", line)

            if match:
                agent_name = match.group(1)

                if agent_name in self.runtimes and agent_name != "orchestrator":
                    # ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                    if session_id:
                        check_cancelled(session_id)

                    # ã“ã®è¡Œã‹ã‚‰å§‹ã¾ã‚‹æŒ‡ç¤ºã‚’åé›†ï¼ˆæ¬¡ã®@ã¾ã§ã€ç©ºè¡Œã¯å«ã‚ã‚‹ï¼‰
                    # å…ˆé ­ã®ã‚³ãƒ­ãƒ³ã‚„ç©ºç™½ã‚’ç¢ºå®Ÿã«é™¤å»
                    initial_instruction = match.group(2).lstrip(': ').strip()
                    instruction_lines = [initial_instruction] if initial_instruction else []
                    i += 1
                    consecutive_empty = 0
                    start_line_index = i - 1  # @agent-name è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    while i < len(lines):
                        next_line = lines[i]
                        # æ¬¡ã®@ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§çµ‚äº†
                        if re.match(r"^@[\w-]+", next_line):
                            break
                        # é€£ç¶šã—ãŸç©ºè¡ŒãŒ2ã¤ä»¥ä¸Šã§çµ‚äº†ï¼ˆæ®µè½åŒºåˆ‡ã‚Šï¼‰
                        if next_line.strip() == "":
                            consecutive_empty += 1
                            if consecutive_empty >= 2:
                                break
                        else:
                            consecutive_empty = 0
                        instruction_lines.append(next_line)
                        i += 1

                    instruction = '\n'.join(instruction_lines).strip()
                    if instruction:
                        delegations.append((agent_name, instruction, start_line_index, i))
                    continue

            i += 1

        # å§”è­²ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Œã°ä¸¦åˆ—å®Ÿè¡Œ
        delegation_results = {}  # {start_line_index: (sub_response, error)}
        delegated_count = len(delegations)

        if delegations:
            # å„å§”è­²ã‚¿ã‚¹ã‚¯ã®ã‚³ãƒ«ãƒ¼ãƒãƒ³ã‚’ä½œæˆ
            async def execute_delegation(agent_name: str, instruction: str, line_index: int):
                try:
                    # å§”è­²å…ˆã¯å¸¸ã«è¡¨ç¤ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‡¦ç†ã®æµã‚Œã‚’æŠŠæ¡ã§ãã‚‹ã‚ˆã†ã«ï¼‰
                    _log_delegation(agent_name, instruction)
                    sub_response = await self._delegate_to_agent(agent_name, instruction, session_id)
                    return (line_index, sub_response, None)
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ä»–ã®ã‚¿ã‚¹ã‚¯ã¯ç¶™ç¶š
                    error_msg = f"@{agent_name} ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                    logging.error(error_msg, exc_info=True)
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®å¿œç­”ã‚’è¿”ã—ã€ã‚¨ãƒ©ãƒ¼æƒ…å ±ã¯åˆ¥é€”è¨˜éŒ²
                    return (line_index, "", error_msg)

            # å…¨ã¦ã®å§”è­²ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            tasks = [
                execute_delegation(agent_name, instruction, start_idx)
                for agent_name, instruction, start_idx, end_idx in delegations
            ]
            results = await asyncio.gather(*tasks, return_exceptions=False)

            # çµæœã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒãƒƒãƒ”ãƒ³ã‚°
            for line_idx, sub_response, error in results:
                delegation_results[line_idx] = (sub_response, error)

        # ç¬¬2ãƒ‘ã‚¹: çµæœã‚’å…ƒã®ä½ç½®ã«ãƒãƒ¼ã‚¸ã—ã¦æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’æ§‹ç¯‰
        processed_lines = []
        successful_results = []  # ã‚µãƒãƒªãƒ¼ç”Ÿæˆç”¨ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’é™¤ãï¼‰
        i = 0
        delegation_idx = 0

        while i < len(lines):
            # ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
            if session_id:
                check_cancelled(session_id)

            # å§”è­²ãŒã‚ã‚‹ä½ç½®ã‹ãƒã‚§ãƒƒã‚¯
            if delegation_idx < len(delegations):
                _, _, start_idx, end_idx = delegations[delegation_idx]
                if i == start_idx:
                    # ã“ã®ä½ç½®ã¯å§”è­²ã‚¿ã‚¹ã‚¯ã®é–‹å§‹ä½ç½®
                    sub_response, error = delegation_results[start_idx]
                    if error:
                        processed_lines.append(f"### ã‚¨ãƒ©ãƒ¼\n{error}")
                    else:
                        processed_lines.append(sub_response)
                        successful_results.append(sub_response)
                    i = end_idx
                    delegation_idx += 1
                    continue

            processed_lines.append(lines[i])
            i += 1

        # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å§”è­²ãŒã‚ã£ãŸå ´åˆã€æœ€çµ‚ã¾ã¨ã‚ã‚’ç”Ÿæˆ
        if successful_results:
            summary = await self._generate_final_summary(successful_results, session_id)
            if summary:
                processed_lines.append(f"\n---\n## ã¾ã¨ã‚\n{summary}")

        return '\n'.join(processed_lines)

    async def _generate_final_summary(self, results: list, session_id: Optional[str] = None) -> str:
        """ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çµæœã‹ã‚‰æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’ LLM ã§ç”Ÿæˆ"""
        # çµæœã‚’çŸ­ãåˆ‡ã‚Šè©°ã‚
        combined = '\n'.join(results)
        if len(combined) > 2000:
            combined = combined[:2000] + "...(çœç•¥)"

        # Orchestrator ã«è¦è¦ç´„ã‚’ä¾é ¼
        runtime = self.runtimes.get("orchestrator")
        if not runtime:
            return "å®Œäº†ã—ã¾ã—ãŸã€‚"

        prompt = f"""ä»¥ä¸‹ã®ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã«3-5è¡Œã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
æŠ€è¡“çš„è©³ç´°ã¯çœç•¥ã—ã€ã€Œä½•ãŒå®Œäº†ã—ãŸã‹ã€ã€Œçµæœã¯ã©ã†ã ã£ãŸã‹ã€ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚

---
{combined}
---

ã¾ã¨ã‚ï¼ˆ3-5è¡Œï¼‰:"""

        try:
            summary = await runtime.run(prompt, history=None, session_id=session_id)
            # summaryãŒNoneã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
            if summary is None:
                return ""
            # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            lines = summary.strip().split('\n')
            if len(lines) > 7:
                summary = '\n'.join(lines[:7])
            return summary.strip()
        except Exception:
            return "ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"

    async def _delegate_to_agent(
        self,
        agent_name: str,
        query: str,
        parent_session_id: Optional[str] = None
    ) -> str:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å‡¦ç†ã‚’å§”è­²ã™ã‚‹ã€‚
        ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç‹¬ç«‹ã—ãŸå±¥æ­´ã‚’æŒã¤ã€‚
        """
        # é€²æ—é€šçŸ¥
        if self.progress_callback:
            self.progress_callback(
                event_type="delegate",
                name=agent_name,
                detail=query,
                agent_name=agent_name,
                parent_agent=self.name,
                status="running"
            )

        runtime = self.runtimes[agent_name]

        try:
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
            if parent_session_id:
                check_cancelled(parent_session_id)

            # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—/ä½œæˆ
            if parent_session_id:
                sub_session_id = self._get_or_create_sub_session(parent_session_id, agent_name)

                # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å±¥æ­´ã‚’å–å¾—ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
                history_format = "openai" if self.provider in (LLMProvider.OPENAI, LLMProvider.OPENROUTER) else "gemini"
                sub_history = self.session_logger.get_agent_history(
                    sub_session_id,
                    limit=10,  # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯çŸ­ã‚ã®å±¥æ­´
                    format=history_format
                )

                # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆOrchestratorï¼‰ã‹ã‚‰ã®ã‚¯ã‚¨ãƒªã‚’è¨˜éŒ²
                self.session_logger.log_agent_message(
                    sub_session_id,
                    "user",
                    query,
                    agent_id="orchestrator"
                )
            else:
                sub_history = None
                sub_session_id = None

            # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œï¼ˆç‹¬è‡ªã®å±¥æ­´ã§ï¼‰
            # AgentRuntime.run ã¯åŒæœŸçš„ãªã®ã§ã€ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            
            # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæƒ…å ±ã‚’è‡ªå‹•æ³¨å…¥
            working_dir_prefix = self._get_working_context_prompt()
            query_with_workdir = working_dir_prefix + query
            
            # è©•ä¾¡æŒ‡ç¤ºã¯è¿½åŠ ã—ãªã„ï¼ˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒäº‹å¾Œè©•ä¾¡ã‚’è¡Œã†ãŸã‚ï¼‰
            enhanced_query = query_with_workdir
            
            # Skills æ³¨å…¥: Orchestrator ãŒãƒ„ãƒ¼ãƒ«ã§ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚¹ã‚­ãƒ«ã‚’ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å…±æœ‰
            loaded_skills = get_loaded_skills()
            if loaded_skills:
                # ã‚¿ã‚¹ã‚¯ã«é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ«ã‚’é¸æŠ
                relevant_skills = []
                for name, skill in loaded_skills.items():
                    if skill.matches_input(query):
                        relevant_skills.append(skill)
                
                # ãƒãƒƒãƒã—ãªãã¦ã‚‚å…¨ã‚¹ã‚­ãƒ«ã‚’æ¸¡ã™ï¼ˆOrchestrator ãŒé¸ã‚“ã ã‚‚ã®ï¼‰
                if not relevant_skills:
                    relevant_skills = list(loaded_skills.values())[:3]  # æœ€å¤§3ã¤
                
                if relevant_skills:
                    runtime.skills = relevant_skills
                    if self.verbose:
                        print(f"[Skills] Injected {len(relevant_skills)} skills from pool to @{agent_name}: {[s.name for s in relevant_skills]}")
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬é–‹å§‹
            agent_start_time = time.time()
            
            response = await runtime.run(enhanced_query, history=sub_history)
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬çµ‚äº†
            agent_execution_time_ms = int((time.time() - agent_start_time) * 1000)
            
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
            runtime_metrics = runtime.get_metrics()

            # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è©•ä¾¡
            orchestrator_eval = self._evaluate_subagent_response(agent_name, query, response)
            eval_block = ""
            if orchestrator_eval:
                self._inline_evaluations[agent_name] = orchestrator_eval
                if self.verbose:
                    print(f"[Orchestrator] Saved evaluation for @{agent_name}: {orchestrator_eval}")
                
                eval_block = f"""
---
ã€ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã€‘
- completion: {orchestrator_eval.get('completion', 0)}
- quality: {orchestrator_eval.get('quality', 0)}
- task_complexity: {orchestrator_eval.get('task_complexity', 0)}
- prompt_specificity: {orchestrator_eval.get('prompt_specificity', 0)}
"""
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã®è¦ç´„æƒ…å ±ã‚’å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ã‚’æ­¢ã‚ãªã„ï¼‰
            # ãƒ¡ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆparent_session_idï¼‰ã® summary_depth ã‚’ä½¿ç”¨
            # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã¨ã—ã¦æ‰±ã†
            agent_summary_depth = 0
            agent_history_turns = 0
            try:
                # ãƒ¡ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„æ·±åº¦ã‚’å–å¾—ï¼ˆã‚µãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã¯ãªãï¼‰
                main_session = parent_session_id or sub_session_id
                if main_session and self.session_logger:
                    agent_summary_depth = self.session_logger.get_summary_depth(main_session)
                    agent_history = self.session_logger.get_agent_history(main_session, limit=100)
                    agent_history_turns = len(agent_history) if agent_history else 0
            except Exception:
                pass  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—å¤±æ•—ã¯ç„¡è¦–
            
            from moco.core.optimizer import AgentExecutionMetrics
            inline_score = None
            if orchestrator_eval:
                completion = float(orchestrator_eval.get("completion", 0))
                quality = float(orchestrator_eval.get("quality", 0))
                inline_score = (completion + quality) / 20.0

            agent_metric = AgentExecutionMetrics(
                agent_name=agent_name,
                parent_agent=self.name,
                tokens_input=runtime_metrics.get("prompt_tokens", 0),
                tokens_output=runtime_metrics.get("candidates_tokens", runtime_metrics.get("completion_tokens", 0)),
                execution_time_ms=agent_execution_time_ms,
                tool_calls=response.count("ğŸ› ï¸") if response else 0,  # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®æ¨å®š
                inline_score=inline_score,  # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼è©•ä¾¡ã‚’åæ˜ 
                # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®è©³ç´°é …ç›®
                eval_completion=int(orchestrator_eval.get("completion", 0)) if orchestrator_eval else None,
                eval_quality=int(orchestrator_eval.get("quality", 0)) if orchestrator_eval else None,
                eval_task_complexity=int(orchestrator_eval.get("task_complexity", 0)) if orchestrator_eval else None,
                eval_prompt_specificity=int(orchestrator_eval.get("prompt_specificity", 0)) if orchestrator_eval else None,
                # è¦ç´„ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£
                summary_depth=agent_summary_depth,
                history_turns=agent_history_turns,
                error_message=None
            )
            self._agent_execution_metrics.append(agent_metric)

            if self.progress_callback:
                self.progress_callback(
                    event_type="delegate",
                    name=agent_name,
                    detail=query,
                    agent_name=agent_name,
                    parent_agent=self.name,
                    status="completed"
                )

            # ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¿œç­”ã‚’è¨˜éŒ²
            if sub_session_id:
                self.session_logger.log_agent_message(
                    sub_session_id,
                    "assistant",
                    response,
                    agent_id=agent_name
                )

            return f"@{agent_name}: {response}{eval_block}"

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
            from moco.core.optimizer import AgentExecutionMetrics
            agent_metric = AgentExecutionMetrics(
                agent_name=agent_name,
                parent_agent=self.name,
                error_message=str(e)
            )
            self._agent_execution_metrics.append(agent_metric)
            return f"Error running agent @{agent_name}: {e}"

    def _list_agents(self) -> str:
        if not self.agents:
            return f"No agents loaded. Please check profiles/{self.profile}/agents/*.md files."

        lines = ["Available agents:"]
        for name, config in self.agents.items():
            lines.append(f"- @{name}: {config.description}")
        return "\n".join(lines)

    def _get_working_context_prompt(self) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã™ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
        display_dir = Path(self.working_directory).name
        return (
            f"ã€ä½œæ¥­ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹: ./{display_dir}\n"
            f"â›” ã“ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å¤–ã«å‡ºã‚‹ã“ã¨ã¯ç¦æ­¢ã€‚ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã¯å¿…ãšã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§è¡Œã†ã“ã¨ã€‚\n\n"
        )

    def _evaluate_subagent_response(
        self,
        agent_name: str,
        task: str,
        response: str
    ) -> Optional[dict]:
        """ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒè©•ä¾¡

        Args:
            agent_name: ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå
            task: ä¾é ¼ã—ãŸã‚¿ã‚¹ã‚¯
            response: ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

        Returns:
            è©•ä¾¡çµæœï¼ˆè¾æ›¸ï¼‰ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
        """
        eval_prompt = f"""ä»¥ä¸‹ã®ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœã‚’å®¢è¦³çš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã€‘{agent_name}

ã€ä¾é ¼ã‚¿ã‚¹ã‚¯ã€‘
{task}

ã€å®Ÿè¡Œçµæœã€‘
{response[:2000]}

---

è©•ä¾¡åŸºæº–ï¼ˆ0-10ã®æ•´æ•°ã§æ¡ç‚¹ï¼‰:

1. completion (0-10):
   - 9-10: ã‚¨ãƒ©ãƒ¼ãªãã‚¿ã‚¹ã‚¯ãŒå®Œé‚ã—ã€ç›®çš„ãŒå®Œå…¨ã«é”æˆã•ã‚ŒãŸ
   - 4-8: ã‚¿ã‚¹ã‚¯ã®å¤§éƒ¨åˆ†ãŒé”æˆã•ã‚ŒãŸãŒã€ä¸€éƒ¨ä¸å®Œå…¨ã¾ãŸã¯æ”¹å–„ã®ä½™åœ°ã‚ã‚Š
   - 1-3: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã€ã¾ãŸã¯ç›®çš„ãŒæœªé”æˆ
   - 0: é€”ä¸­ã§æ–­å¿µã—ãŸã€ã¾ãŸã¯è§£æ±ºç­–ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸ

2. quality (0-10):
   - é«˜å¾—ç‚¹: ã‚³ãƒ¼ãƒ‰ã®æ­£ç¢ºæ€§ã€åŠ¹ç‡æ€§ã€å®‰å…¨æ€§ãŒé«˜ã„ã€‚èª¬æ˜ãŒæ˜ç¢ºã§ååˆ†
   - ä½å¾—ç‚¹: ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€ã¾ãŸã¯èª¬æ˜ãŒä¸ååˆ†ã€ã‚³ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚‹

3. task_complexity (1-10): ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•

4. prompt_specificity (0-10): ä¾é ¼ã®æŒ‡ç¤ºã®å…·ä½“æ€§ï¼ˆã‚¿ã‚¹ã‚¯ã®é›£ã—ã•ã§ã¯ãªãã€æŒ‡ç¤ºãŒã©ã‚Œã ã‘æ˜ç¢ºã ã£ãŸã‹ï¼‰

ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ãªã„ã§ãã ã•ã„ï¼‰:

{{
  "completion": <æ•´æ•°>,
  "quality": <æ•´æ•°>,
  "task_complexity": <æ•´æ•°>,
  "prompt_specificity": <æ•´æ•°>
}}"""

        try:
            result = self._llm_generate_for_optimizer(
                prompt=eval_prompt,
                model=os.environ.get("GEMINI_MODEL", "gemini-2.0-flash"),
                max_tokens=200,
                temperature=0.3
            )

            # JSONãƒ‘ãƒ¼ã‚¹ï¼ˆresult ãŒ None ã®å ´åˆã‚‚è€ƒæ…®ï¼‰
            result = (result or "").strip()
            eval_json = SmartJSONParser.parse(result, default={})

            # æ•´æ•°å€¤ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç¯„å›²ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            for key in ["completion", "quality", "prompt_specificity"]:
                if key in eval_json:
                    try:
                        value = int(eval_json[key])
                        eval_json[key] = max(0, min(10, value))  # 0-10ã®ç¯„å›²ã«åˆ¶ç´„
                    except (ValueError, TypeError):
                        eval_json[key] = 0
            if "task_complexity" in eval_json:
                try:
                    value = int(eval_json["task_complexity"])
                    eval_json["task_complexity"] = max(1, min(10, value))  # 1-10ã®ç¯„å›²ã«åˆ¶ç´„
                except (ValueError, TypeError):
                    eval_json["task_complexity"] = 5

            if self.verbose:
                print(f"[Orchestrator] Evaluated @{agent_name}: {eval_json}")

            return eval_json

        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"[Orchestrator] Failed to evaluate @{agent_name}: {e}")
            if self.verbose:
                print(f"[Orchestrator] Failed to evaluate @{agent_name}: {e}")
            return None

    def _aggregate_inline_evaluations(self) -> dict:
        """åé›†ã—ãŸã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’é›†è¨ˆ"""
        if not self._inline_evaluations:
            return {}
        
        total = {"completion": 0, "quality": 0, "task_complexity": 0, "prompt_specificity": 0}
        count = len(self._inline_evaluations)
        
        for eval_data in self._inline_evaluations.values():
            for key in total:
                total[key] += eval_data.get(key, 5)
        
        return {key: val / count for key, val in total.items()}

    def create_session(self, title: str = "New Session", **metadata) -> str:
        """æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        return self.session_logger.create_session(
            profile=self.profile,
            title=title,
            **metadata
        )

    def get_session_history(self, session_id: str, limit: int = 20) -> List[Dict[str, Any]]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—"""
        return self.session_logger.get_agent_history(session_id, limit=limit)

    def continue_session(self, session_id: str) -> None:
        """æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¶™ç¶šã™ã‚‹éš›ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã™ã‚‹"""
        saved_profile = self.session_logger.get_session_profile(session_id)
        if saved_profile != self.profile and self.verbose:
            print(f'Warning: Session was created with profile "{saved_profile}", but current profile is "{self.profile}"')


    def get_sub_session_id(self, parent_session_id: str, agent_name: str) -> Optional[str]:
        """ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å–å¾—"""
        if parent_session_id in self._sub_sessions:
            return self._sub_sessions[parent_session_id].get(agent_name)
        return None

    def get_all_sub_sessions(self, parent_session_id: str) -> Dict[str, str]:
        """è¦ªã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ç´ã¥ãå…¨ã‚µãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
        return self._sub_sessions.get(parent_session_id, {})
    
    # ========== Optimizer API ==========
    
    def _llm_generate_for_optimizer(
        self,
        prompt: str,
        model: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """Optimizerç”¨ã®LLMç”Ÿæˆé–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ»åŒæœŸç‰ˆï¼‰
        
        å„ªå…ˆé †ä½: ZAI â†’ Gemini â†’ OpenRouter â†’ OpenAI
        ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ™‚ã¯æ¬¡ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        Note: ã“ã®é–¢æ•°ã¯åŒæœŸçš„ã§ã™ã€‚TaskAnalyzer/QualityEvaluatorå†…ã§
        run_in_executor ã‚’ä½¿ç”¨ã—ã¦éåŒæœŸã«å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚
        """
        errors = []
        
        # 1. ZAI ã‚’è©¦ã™ï¼ˆOpenAIäº’æ›APIï¼‰
        zai_key = os.environ.get("ZAI_API_KEY")
        if zai_key:
            try:
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=zai_key,
                    base_url="https://api.z.ai/api/coding/paas/v4"
                )
                response = client.chat.completions.create(
                    model=os.environ.get("ZAI_MODEL", "glm-4.7"),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content or ""
            except Exception as e:
                errors.append(f"ZAI: {e}")
                if "429" not in str(e) and "rate" not in str(e).lower():
                    raise  # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã« raise
        
        # 2. Gemini ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        gemini_key = (
            os.environ.get("GENAI_API_KEY") or
            os.environ.get("GEMINI_API_KEY") or
            os.environ.get("GOOGLE_API_KEY")
        )
        if gemini_key:
            try:
                from google import genai
                from google.genai import types
                
                client = genai.Client(api_key=gemini_key)
                response = client.models.generate_content(
                    model=os.environ.get("GEMINI_MODEL", "gemini-2.0-flash"),
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        max_output_tokens=max_tokens,
                        temperature=temperature
                    )
                )
                return response.text
            except Exception as e:
                errors.append(f"Gemini: {e}")
                if "429" not in str(e) and "RESOURCE_EXHAUSTED" not in str(e):
                    raise  # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã« raise
        
        # 3. OpenRouter ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        openrouter_key = os.environ.get("OPENROUTER_API_KEY")
        if openrouter_key:
            try:
                from openai import OpenAI
                
                client = OpenAI(
                    api_key=openrouter_key,
                    base_url="https://openrouter.ai/api/v1"
                )
                response = client.chat.completions.create(
                    model=os.environ.get("OPENROUTER_MODEL", "xiaomi/mimo-v2-flash"),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content or ""
            except Exception as e:
                errors.append(f"OpenRouter: {e}")
        
        # 4. OpenAI ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        openai_key = os.environ.get("OPENAI_API_KEY")
        if openai_key:
            try:
                from openai import OpenAI
                
                client = OpenAI(api_key=openai_key)
                model_name = os.environ.get("OPENAI_MODEL", "gpt-5.2-codex")
                # gpt-5ç³»ã¯ max_completion_tokens ã‚’ä½¿ç”¨
                if "gpt-5" in model_name or "o1" in model_name or "o3" in model_name:
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "user", "content": prompt}],
                        max_completion_tokens=max_tokens,
                        temperature=temperature
                    )
                else:
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                return response.choices[0].message.content or ""
            except Exception as e:
                errors.append(f"OpenAI: {e}")
        
        # å…¨ã¦å¤±æ•—
        raise RuntimeError(f"All LLM providers failed: {'; '.join(errors)}")
    
    def get_optimizer_stats(self, days: int = 30) -> Dict[str, Any]:
        """Optimizer ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return self.quality_tracker.get_stats(profile=self.profile, days=days)
    
    def get_optimizer_recommendations(self) -> List[str]:
        """Optimizer ã®æ¨å¥¨äº‹é …ã‚’å–å¾—"""
        from .optimizer import AutoTuner
        tuner = AutoTuner(self.quality_tracker, self.optimizer_config)
        return tuner.get_recommendations()
    
    def run_optimizer_tuning(self) -> Dict[str, Any]:
        """Optimizer ã®è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        from .optimizer import AutoTuner
        tuner = AutoTuner(self.quality_tracker, self.optimizer_config)
        result = tuner.tune()
        return {
            "status": result.status,
            "reason": result.reason,
            "old_thresholds": result.old_thresholds,
            "new_thresholds": result.new_thresholds,
            "samples_used": result.samples_used
        }
    
    def get_last_selection(self) -> Optional[Dict[str, Any]]:
        """æœ€å¾Œã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠçµæœã‚’å–å¾—"""
        if self._current_selection:
            return {
                "depth": self._current_selection.depth,
                "agents": self._current_selection.agents,
                "skipped": self._current_selection.skipped,
                "reason": self._current_selection.reason,
                "scores": self._current_scores
            }
        return None

    async def run(self, user_input: str, session_id: Optional[str] = None) -> str:
        """
        éåŒæœŸã§ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆasyncioå¯¾å¿œï¼‰

        Args:
            user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆå±¥æ­´ç®¡ç†ç”¨ï¼‰
                        çœç•¥æ™‚ã¯æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ

        Returns:
            ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®å¿œç­”
        """
        session_id, history = self._prepare_session(user_input, session_id)
        try:
            return await self.process_message(user_input, session_id, history)
        except OperationCancelled:
            if session_id:
                clear_cancel_event(session_id)
            return f"Job {session_id} was cancelled."
        except Exception as e:
            if session_id:
                clear_cancel_event(session_id)
            raise e
        finally:
            if session_id:
                # æ­£å¸¸çµ‚äº†æ™‚ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆäºŒé‡ã«å‘¼ã‚“ã§ã‚‚å®‰å…¨ï¼‰
                clear_cancel_event(session_id)

    async def process_message_stream(
        self,
        user_input: str,
        session_id: Optional[str] = None
    ):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†ã—ã€çµæœã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¿”ã™ï¼ˆSSEç”¨ï¼‰
        """
        queue = asyncio.Queue()
        loop = asyncio.get_event_loop()

        def stream_callback(event_type: str, **kwargs):
            data = {"type": event_type}
            data.update(kwargs)
            loop.call_soon_threadsafe(queue.put_nowait, data)

        # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä¿å­˜ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š
        original_callback = self.progress_callback
        self.progress_callback = stream_callback
        
        # å…¨ã¦ã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ›´æ–°
        for runtime in self.runtimes.values():
            runtime.progress_callback = stream_callback

        # å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        task = asyncio.create_task(self.run(user_input, session_id))

        try:
            while not task.done() or not queue.empty():
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—ã—ã¦ yield
                    item = await asyncio.wait_for(queue.get(), timeout=0.1)
                    yield item
                except (asyncio.TimeoutError, asyncio.QueueEmpty):
                    continue
            
            # æœ€çµ‚çµæœã‚’ç¢ºèªï¼ˆä¾‹å¤–ãŒã‚ã‚Œã°ã“ã“ã§ç™ºç”Ÿã™ã‚‹ï¼‰
            await task
            
        finally:
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å…ƒã«æˆ»ã™
            self.progress_callback = original_callback
            for runtime in self.runtimes.values():
                runtime.progress_callback = original_callback

    def run_sync(self, user_input: str, session_id: Optional[str] = None) -> str:
        """
        åŒæœŸçš„ã«ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹
        """
        session_id, history = self._prepare_session(user_input, session_id)

        # æ‰‹å‹•ã§ãƒ«ãƒ¼ãƒ—ã‚’ç®¡ç†ã—ã¦ KeyboardInterrupt ã‚’é©åˆ‡ã«å‡¦ç†
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(self.process_message(user_input, session_id, history))
            return result
        except KeyboardInterrupt:
            # ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«å®Œäº†ã‚’å¾…ã¤ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ï¼‰
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            raise
        except RuntimeError as e:
            # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã®å ´åˆ
            if "cannot be called from a running event loop" in str(e):
                # nest_asyncio ã‚’è©¦ã¿ã‚‹
                try:
                    import nest_asyncio
                    nest_asyncio.apply()
                    return asyncio.run(self.process_message(user_input, session_id, history))
                except ImportError:
                    pass
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                import threading
                result = [None, None]
                
                def run_in_thread():
                    try:
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        result[0] = new_loop.run_until_complete(
                            self.process_message(user_input, session_id, history)
                        )
                        new_loop.close()
                    except Exception as ex:
                        result[1] = ex
                
                t = threading.Thread(target=run_in_thread)
                t.start()
                t.join(timeout=300)
                
                if result[1]:
                    raise result[1]
                return result[0] or ""
            raise
        except Exception as e:
            if self.verbose:
                print(f"[Orchestrator] run_sync error: {e}")
            return f"Error: {e}"
        finally:
            try:
                # async generator ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                loop.run_until_complete(loop.shutdown_asyncgens())
                loop.close()
            except Exception:
                pass