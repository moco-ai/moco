import asyncio
import os
import json
import inspect
import hashlib
from ..cancellation import check_cancelled
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Callable, Union, get_type_hints
import sys
from collections import defaultdict

from ..tools.skill_loader import SkillConfig


class ToolCallTracker:
    """åŒã˜ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡ºãƒ»é˜²æ­¢ã™ã‚‹ãƒˆãƒ©ãƒƒã‚«ãƒ¼"""
    
    def __init__(self, max_repeats: int = 3, window_size: int = 10):
        """
        Args:
            max_repeats: åŒã˜å‘¼ã³å‡ºã—ãŒè¨±å¯ã•ã‚Œã‚‹æœ€å¤§å›æ•°
            window_size: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ç›´è¿‘ã®å‘¼ã³å‡ºã—æ•°
        """
        self.history: List[str] = []
        self.max_repeats = max_repeats
        self.window_size = window_size
        self.blocked_calls: Dict[str, int] = defaultdict(int)
    
    def _make_key(self, tool_name: str, args: dict) -> str:
        """ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆå¼•æ•°ãŒå¤§ãã„å ´åˆã¯ãƒãƒƒã‚·ãƒ¥åŒ–ï¼‰"""
        try:
            args_str = json.dumps(args, sort_keys=True, default=str)
            # å¼•æ•°ãŒ100æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯MD5ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨
            if len(args_str) > 100:
                args_hash = hashlib.md5(args_str.encode()).hexdigest()
                return f"{tool_name}:hash:{args_hash}"
            return f"{tool_name}:{args_str}"
        except Exception:
            return f"{tool_name}:{str(args)}"
    
    def check_and_record(self, tool_name: str, args: dict) -> tuple[bool, str]:
        """
        ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å±¥æ­´ã«è¨˜éŒ²ã™ã‚‹ã€‚
        
        Returns:
            (allowed: bool, message: str)
            - allowed=True: å®Ÿè¡Œè¨±å¯
            - allowed=False: ãƒ«ãƒ¼ãƒ—æ¤œå‡ºã€å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
        """
        call_key = self._make_key(tool_name, args)
        
        # ç›´è¿‘ã®å‘¼ã³å‡ºã—ã§åŒã˜ã‚­ãƒ¼ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        recent = self.history[-self.window_size:] if len(self.history) >= self.window_size else self.history
        repeat_count = sum(1 for h in recent if h == call_key)
        
        if repeat_count >= self.max_repeats:
            self.blocked_calls[call_key] += 1
            return False, (
                f"âš ï¸ ãƒ«ãƒ¼ãƒ—æ¤œå‡º: {tool_name} ãŒåŒã˜å¼•æ•°ã§ {repeat_count} å›å‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸã€‚\n"
                f"ã“ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚\n"
                f"åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã™ã‹ã€ç•°ãªã‚‹å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
            )
        
        self.history.append(call_key)
        return True, ""
    
    def reset(self):
        """å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.history.clear()
        self.blocked_calls.clear()

# Gemini
from google import genai
from google.genai import types

# OpenAI
try:
    from openai import OpenAI, AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from ..tools.discovery import AgentConfig
from ..storage.semantic_memory import SemanticMemory
from .context_compressor import ContextCompressor

# ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ãƒ­ã‚°ç”¨
MAX_ARG_LEN = 40  # å¼•æ•°ã®æœ€å¤§æ–‡å­—æ•°

class StreamPrintState:
    """stdoutçŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã«ãƒªã‚»ãƒƒãƒˆå¯èƒ½ï¼‰"""
    broken = False
    
    @classmethod
    def reset(cls):
        """ãƒ†ã‚¹ãƒˆç”¨ï¼šçŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        cls.broken = False


def _safe_stream_print(text: str) -> None:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºç”¨ã®å®‰å…¨ãª printã€‚

    Web UI å®Ÿè¡Œæ™‚ãªã©ã§ stdout ãŒé–‰ã˜ã‚‰ã‚Œã¦ã„ã‚‹ã¨ BrokenPipeError / OSError(errno=32) ãŒå‡ºã‚‹ãŸã‚ã€
    ä»¥å¾Œã®æ¨™æº–å‡ºåŠ›ã‚’æŠ‘åˆ¶ã—ã¦å‡¦ç†è‡ªä½“ã¯ç¶™ç¶šã™ã‚‹ã€‚
    """
    if StreamPrintState.broken:
        return
    try:
        print(text, end="", flush=True)
    except BrokenPipeError:
        StreamPrintState.broken = True
    except OSError as e:
        if getattr(e, "errno", None) == 32:
            StreamPrintState.broken = True
            return
        raise

# ãƒ„ãƒ¼ãƒ«ç¨®é¡åˆ¥ã®ã‚¢ã‚¤ã‚³ãƒ³ã¨è‰²
TOOL_STYLES = {
    # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
    "read_file": ("ğŸ“–", "cyan"),
    "write_file": ("ğŸ“", "green"),
    "edit_file": ("âœï¸", "yellow"),
    # å®Ÿè¡Œç³»
    "execute_bash": ("âš¡", "magenta"),
    # å§”è­²
    "delegate_to_agent": ("ğŸ‘¤", "blue"),
    # æ¤œç´¢ç³»
    "websearch": ("ğŸ”", "cyan"),
    "webfetch": ("ğŸŒ", "cyan"),
    "grep": ("ğŸ”", "dim"),
    "codebase_search": ("ğŸ”", "cyan"),
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    "list_dir": ("ğŸ“", "dim"),
    "glob_search": ("ğŸ“‚", "dim"),
    # è¨ˆç®—ç³»
    "calculate_tax": ("ğŸ§®", "green"),
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    "_default": ("ğŸ”§", "dim"),
}

def _format_tool_log(tool_name: str, args: dict) -> tuple:
    """ãƒ„ãƒ¼ãƒ«ãƒ­ã‚°ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‚(icon, name, arg_str, color) ã‚’è¿”ã™"""
    style = TOOL_STYLES.get(tool_name, TOOL_STYLES["_default"])
    icon, color = style

    # å¼•æ•°ã‚’æŠ½å‡º
    arg_str = ""

    if tool_name in ("read_file", "write_file", "edit_file"):
        path = args.get("path") or args.get("file_path") or ""
        # ãƒ•ã‚¡ã‚¤ãƒ«åã ã‘æŠ½å‡º
        if path and "/" in path:
            arg_str = path.split("/")[-1]
        else:
            arg_str = path or ""
        # offset/limit ã‚‚è¡¨ç¤ºï¼ˆä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆï¼‰
        offset = args.get("offset")
        limit = args.get("limit")
        if offset or limit:
            start = offset or 1
            end = start + (limit or 0) - 1 if limit else "end"
            arg_str += f" [L{start}-{end}]"

    elif tool_name == "execute_bash":
        cmd = args.get("command") or ""
        # æœ€åˆã®ã‚³ãƒãƒ³ãƒ‰ã ã‘
        arg_str = cmd.split()[0] if cmd and cmd.split() else (cmd or "")
        if cmd and len(cmd) > len(arg_str) + 5:
            arg_str += " ..."

    elif tool_name == "delegate_to_agent":
        arg_str = f"@{args.get('agent_name') or ''}"

    elif tool_name in ("websearch", "codebase_search"):
        q = args.get("query") or ""
        arg_str = q[:30] + "..." if len(q) > 30 else q

    elif tool_name == "grep":
        pattern = args.get("pattern") or ""
        arg_str = pattern[:20]

    elif tool_name in ("list_dir", "glob_search"):
        path = args.get("target_dir") or args.get("path") or ""
        if path and "/" in path:
            arg_str = path.split("/")[-1] or path.split("/")[-2] or ""
        else:
            arg_str = path or ""

    elif tool_name == "webfetch":
        url = args.get("url") or ""
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ã ã‘
        if url and "://" in url:
            arg_str = url.split("://")[1].split("/")[0]
        else:
            arg_str = url[:25] if url else ""

    else:
        # ãã®ä»–: æœ€åˆã®å¼•æ•°
        for k, v in list(args.items())[:1]:
            v_str = str(v)
            if len(v_str) > 25:
                v_str = v_str[:25] + "..."
            arg_str = v_str
            break

    # é•·ã•åˆ¶é™
    if len(arg_str) > MAX_ARG_LEN:
        arg_str = arg_str[:MAX_ARG_LEN - 3] + "..."

    return icon, tool_name, arg_str, color

try:
    from rich.console import Console
    _tool_console = Console()

    def _log_tool_use(tool_name: str, args: dict = None, verbose: bool = False):
        """ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚’ç°¡æ½”ã«ãƒ­ã‚°"""
        if verbose:
            pass  # verbose ã®å ´åˆã¯è©³ç´°ãƒ­ã‚°ãŒåˆ¥é€”å‡ºåŠ›ã•ã‚Œã‚‹
        else:
            icon, name, arg_str, color = _format_tool_log(tool_name, args or {})
            # ãƒ„ãƒ¼ãƒ«åã‚’å›ºå®šå¹…ã§æƒãˆã‚‹
            name_padded = name[:18].ljust(18)
            if arg_str:
                _tool_console.print(f"    {icon} [{color}]{name_padded}[/{color}] [dim]â†’ {arg_str}[/dim]")
            else:
                _tool_console.print(f"    {icon} [{color}]{name_padded}[/{color}]")
except ImportError:
    def _log_tool_use(tool_name: str, args: dict = None, verbose: bool = False):
        if not verbose:
            icon, name, arg_str, color = _format_tool_log(tool_name, args or {})
            name_padded = name[:18].ljust(18)
            if arg_str:
                print(f"    {icon} {name_padded} â†’ {arg_str}")
            else:
                print(f"    {icon} {name_padded}")


def _validate_arguments(func: Callable, args: Dict[str, Any]) -> Dict[str, Any]:
    """
    é–¢æ•°ã®å¼•æ•°ã®å‹ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€å¯èƒ½ãªé™ã‚Šå¤‰æ›ã™ã‚‹ã€‚
    """
    sig = inspect.signature(func)
    try:
        hints = get_type_hints(func)
    except Exception:
        hints = {}

    validated_args = {}
    for param_name, param in sig.parameters.items():
        if param_name in ("self", "cls"):
            continue
        
        if param_name in args:
            val = args[param_name]
            expected_type = hints.get(param_name)
            
            if expected_type:
                # ç°¡æ˜“çš„ãªå‹å¤‰æ›/ãƒã‚§ãƒƒã‚¯
                if expected_type is int and not isinstance(val, int):
                    try:
                        val = int(val)
                    except (ValueError, TypeError):
                        pass
                elif expected_type is float and not isinstance(val, (int, float)):
                    try:
                        val = float(val)
                    except (ValueError, TypeError):
                        pass
                elif expected_type is bool and not isinstance(val, bool):
                    if str(val).lower() in ("true", "1", "yes"):
                        val = True
                    elif str(val).lower() in ("false", "0", "no"):
                        val = False
            
            validated_args[param_name] = val
        elif param.default != inspect.Parameter.empty:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒã‚ã‚‹å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆfunc(**validated_args)ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒä½¿ã‚ã‚Œã‚‹ï¼‰
            pass
            
    return validated_args


def _execute_tool_safely(func: Callable, args: Dict[str, Any]) -> Any:
    """
    ãƒ„ãƒ¼ãƒ«ã‚’å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹ã€‚asyncé–¢æ•°ã®å ´åˆã¯åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ã€‚
    """
    valid_args = _validate_arguments(func, args)
    result = func(**valid_args)
    
    # async é–¢æ•°ã®å ´åˆã¯åŒæœŸçš„ã«å®Ÿè¡Œ
    if asyncio.iscoroutine(result):
        try:
            # Python 3.10+: get_running_loop()ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œä¸­ã®ãƒ«ãƒ¼ãƒ—ã‚’ç¢ºèª
            loop = asyncio.get_running_loop()
            # å®Ÿè¡Œä¸­ã®ãƒ«ãƒ¼ãƒ—å†…ã‹ã‚‰ã¯æ–°ã—ã„ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, result)
                result = future.result(timeout=60)
        except RuntimeError:
            # ãƒ«ãƒ¼ãƒ—ãŒå®Ÿè¡Œä¸­ã§ãªã„å ´åˆã¯ asyncio.run() ã‚’ä½¿ç”¨
            result = asyncio.run(result)
    
    return result


async def _execute_tool_safely_async(func: Callable, args: Dict[str, Any]) -> Any:
    """
    ãƒ„ãƒ¼ãƒ«ã‚’å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹ï¼ˆasyncç‰ˆï¼‰ã€‚
    - async tool / coroutine æˆ»ã‚Šå€¤ã¯ await ã§è§£æ±ºã™ã‚‹
    - åŒæœŸãƒ„ãƒ¼ãƒ«ã¯ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ã‚ˆã† to_thread ã§å®Ÿè¡Œã™ã‚‹
    """
    valid_args = _validate_arguments(func, args)

    # ã¾ãšã¯é€šå¸¸å‘¼ã³å‡ºã—
    if asyncio.iscoroutinefunction(func):
        result = await func(**valid_args)
    else:
        result = await asyncio.to_thread(func, **valid_args)

    # å¿µã®ãŸã‚ coroutine è¿”ã‚Šå€¤ã‚‚è§£æ±º
    if asyncio.iscoroutine(result):
        result = await result
    return result


# ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆã“ã‚Œã‚’è¶…ãˆã‚‹ã¨ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼‰
MAX_TOOL_OUTPUT_CHARS = 50000
_TEMP_OUTPUT_DIR = "/tmp/moco_tool_outputs"

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™ç®¡ç†ï¼ˆ1å›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œå†…ï¼‰
MAX_CONTEXT_TOKENS = 150000      # å…¥åŠ›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸Šé™ï¼ˆç´„150K tokensï¼‰
# MAX_TOOL_CALLS = 15            # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ: ContextCompressor ã§ç®¡ç†
CONTEXT_WARNING_THRESHOLD = 0.8  # 80%ã§è­¦å‘Šãƒ»åœ§ç¸®ç™ºå‹•


def _gemini_messages_to_dict(messages: List[Any]) -> List[Dict[str, Any]]:
    """Gemini ã® types.Content ã‚’ Dict å½¢å¼ã«å¤‰æ›"""
    result = []
    for msg in messages:
        if hasattr(msg, 'role') and hasattr(msg, 'parts'):
            # types.Content ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            parts_text = []
            for part in msg.parts:
                if hasattr(part, 'text') and part.text:
                    parts_text.append(part.text)
                elif hasattr(part, 'function_call'):
                    parts_text.append(f"[function_call: {part.function_call.name}]")
                elif hasattr(part, 'function_response'):
                    parts_text.append(f"[function_response: {part.function_response.name}]")
            result.append({
                "role": msg.role,
                "content": "\n".join(parts_text)
            })
        elif isinstance(msg, dict):
            result.append(msg)
    return result


def _dict_to_gemini_messages(messages: List[Dict[str, Any]]) -> List[Any]:
    """Dict å½¢å¼ã‚’ Gemini ã® types.Content ã«å¤‰æ›"""
    result = []
    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        # model -> assistant ã®å¤‰æ›ï¼ˆGemini ã¯ model ã‚’ä½¿ã†ï¼‰
        if role == "assistant":
            role = "model"
        result.append(types.Content(role=role, parts=[types.Part(text=content)]))
    return result


# å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå…±é€šãƒ«ãƒ¼ãƒ«ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è‡ªå‹•æ³¨å…¥ï¼‰
COMMON_AGENT_RULES = """
## â›” ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä¸Šé™æ™‚ã®ãƒ«ãƒ¼ãƒ«

### è‡ªåˆ†ãŒä¸Šé™ã«é”ã—ãŸå ´åˆ
ã€Œâ›” ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä¸Šé™åˆ°é”ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰:
1. ç¾åœ¨ã¾ã§ã®ä½œæ¥­çµæœã‚’ã¾ã¨ã‚ã‚‹
2. ä»¥ä¸‹ã®JSONå½¢å¼ã§æ®‹ã‚Šã‚¿ã‚¹ã‚¯ã‚’è¿”ã™:

```json
{
  "status": "interrupted",
  "completed": ["å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯1", "å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯2"],
  "remaining": ["æ®‹ã‚Šã‚¿ã‚¹ã‚¯1", "æ®‹ã‚Šã‚¿ã‚¹ã‚¯2"],
  "context": "ç¶™ç¶šã«å¿…è¦ãªæƒ…å ±ï¼ˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã€ç¾åœ¨ã®çŠ¶æ…‹ç­‰ï¼‰"
}
```

### å§”è­²å…ˆã‹ã‚‰ä¸­æ–­ã‚’å—ã‘å–ã£ãŸå ´åˆ
å§”è­²å…ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ `"status": "interrupted"` ã‚’å«ã‚€å¿œç­”ã‚’å—ã‘å–ã£ãŸã‚‰:
1. `remaining` ã®å†…å®¹ã‚’ç¢ºèª
2. åŒã˜ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã« `remaining` ã‚¿ã‚¹ã‚¯ã¨ `context` ã‚’æ¸¡ã—ã¦å†åº¦å§”è­²
3. å®Œäº†ã¾ã§ç¹°ã‚Šè¿”ã™

## ğŸ“„ å‡ºåŠ›ãŒåˆ‡ã‚Šè©°ã‚ã‚‰ã‚ŒãŸå ´åˆã®ãƒ«ãƒ¼ãƒ«

ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ã«ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆ:
- `âš ï¸ OUTPUT TRUNCATED` ã¾ãŸã¯ `Content truncated`
- `ğŸ‘‰ NEXT STEP:` ã§å§‹ã¾ã‚‹æŒ‡ç¤º

**å¿…ãšæŒ‡ç¤ºã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ç¶šãã‚’èª­ã‚“ã§ãã ã•ã„ã€‚**
- åŒã˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç¹°ã‚Šè¿”ã•ãªã„ã“ã¨
- è¡¨ç¤ºã•ã‚ŒãŸãƒ‘ã‚¹ãƒ»offsetãƒ»limit ã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ã“ã¨
- ç¶šãã‚’èª­ã¿çµ‚ã‚ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™ã“ã¨
"""

def _estimate_tokens(text: str) -> int:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®šï¼ˆç°¡æ˜“: 4æ–‡å­— â‰’ 1ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰"""
    return len(text) // 4

def _truncate_tool_output(result: Any, tool_name: str) -> str:
    """
    ãƒ„ãƒ¼ãƒ«å‡ºåŠ›ãŒé•·ã™ãã‚‹å ´åˆã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦å‚ç…§ã‚’è¿”ã™ã€‚
    """
    if result is None:
        return "No output"
    
    result_str = str(result)
    
    if len(result_str) <= MAX_TOOL_OUTPUT_CHARS:
        return result_str
    
    # é•·ã™ãã‚‹å ´åˆã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    import os
    import time
    
    os.makedirs(_TEMP_OUTPUT_DIR, exist_ok=True)
    filename = f"{tool_name}_{int(time.time() * 1000)}.txt"
    filepath = os.path.join(_TEMP_OUTPUT_DIR, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(result_str)
    
    # å…ˆé ­éƒ¨åˆ†ã‚’è¡¨ç¤ºã—ã€æ®‹ã‚Šã¯ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§
    preview = result_str[:500]
    total_lines = result_str.count('\n') + 1
    total_chars = len(result_str)
    
    return (
        f"{preview}\n\n"
        f"âš ï¸ OUTPUT TRUNCATED âš ï¸\n"
        f"Total: {total_chars:,} chars, {total_lines:,} lines\n"
        f"Full output saved to: {filepath}\n\n"
        f"ğŸ‘‰ NEXT STEP: ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ç¶šãã‚’èª­ã‚“ã§ãã ã•ã„:\n"
        f"   read_file(path=\"{filepath}\", offset=1, limit=10000)\n"
    )


def _ensure_jsonable(value: Any) -> Any:
    """JSONåŒ–ã§ããªã„å€¤ï¼ˆcoroutineç­‰ï¼‰ã¯æ–‡å­—åˆ—åŒ–ã—ã¦è¿”ã™ã€‚"""
    try:
        json.dumps(value)
        return value
    except Exception:
        return str(value)


# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
class LLMProvider:
    GEMINI = "gemini"
    OPENAI = "openai"
    OPENROUTER = "openrouter"
    ZAI = "zai"  # Z.ai GLM-4.7


def _is_reasoning_model(model_name: str) -> bool:
    """reasoning/thinking å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    
    - OpenAI: o1, o3, o4 ç³»
    - Gemini (OpenRouterçµŒç”±): gemini-2.5, gemini-3 ç³»
    """
    model_lower = model_name.lower()
    # OpenAI reasoning ãƒ¢ãƒ‡ãƒ«
    openai_patterns = ["o1", "o3", "o4"]
    # Gemini thinking å¯¾å¿œãƒ¢ãƒ‡ãƒ« (OpenRouterçµŒç”±: google/gemini-...)
    gemini_patterns = ["gemini-2.5", "gemini-3", "gemini-2.0-flash-thinking"]
    
    all_patterns = openai_patterns + gemini_patterns
    return any(pattern in model_lower for pattern in all_patterns)


def _python_type_to_schema(py_type) -> Dict[str, Any]:
    """Pythonå‹ã‚’JSONã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›"""
    type_map = {
        str: {"type": "string"},
        int: {"type": "integer"},
        float: {"type": "number"},
        bool: {"type": "boolean"},
        list: {"type": "array"},
        dict: {"type": "object"},
    }
    return type_map.get(py_type, {"type": "string"})


def _func_to_openai_tool(func: Callable, tool_name: str) -> Dict[str, Any]:
    """Pythoné–¢æ•°ã‚’OpenAIã®toolå½¢å¼ã«å¤‰æ›"""
    sig = inspect.signature(func)
    docstring = func.__doc__ or ""
    # docstring å…¨ä½“ã‚’ description ã¨ã—ã¦ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹ãªã©ã‚’ LLM ã«ä¼ãˆã‚‹ï¼‰
    description = docstring.strip() if docstring else f"{tool_name} function"

    properties = {}
    required = []

    try:
        hints = get_type_hints(func)
    except Exception:
        hints = {}

    for param_name, param in sig.parameters.items():
        if param_name in ("self", "cls"):
            continue

        param_type = hints.get(param_name, str)
        schema = _python_type_to_schema(param_type)

        param_desc = f"Parameter: {param_name}"
        if param_name in docstring:
            lines = docstring.split("\n")
            for line in lines:
                if param_name in line and ":" in line:
                    param_desc = line.split(":", 1)[-1].strip()
                    break

        properties[param_name] = {**schema, "description": param_desc}

        if param.default == inspect.Parameter.empty:
            required.append(param_name)

    return {
        "type": "function",
        "function": {
            "name": tool_name,
            "description": description,
            "parameters": {
                "type": "object",
                "properties": properties,
                "required": required,
            }
        }
    }


def _func_to_declaration(func: Callable, tool_name: str) -> types.FunctionDeclaration:
    """Pythoné–¢æ•°ã‚’FunctionDeclarationã«å¤‰æ›"""
    sig = inspect.signature(func)
    docstring = func.__doc__ or ""

    # docstring å…¨ä½“ã‚’ description ã¨ã—ã¦ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹ãªã©ã‚’ LLM ã«ä¼ãˆã‚‹ï¼‰
    description = docstring.strip() if docstring else f"{tool_name} function"

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒã‚’æ§‹ç¯‰
    properties = {}
    required = []

    try:
        hints = get_type_hints(func)
    except Exception:
        hints = {}

    for param_name, param in sig.parameters.items():
        if param_name in ("self", "cls"):
            continue

        # å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        param_type = hints.get(param_name, str)
        schema = _python_type_to_schema(param_type)

        # docstringã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜ã‚’æŠ½å‡ºï¼ˆArgs: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
        param_desc = f"Parameter: {param_name}"
        if f"{param_name}" in docstring:
            # ç°¡æ˜“çš„ãªæŠ½å‡º
            lines = docstring.split("\n")
            for line in lines:
                if param_name in line and ":" in line:
                    param_desc = line.split(":", 1)[-1].strip()
                    break

        properties[param_name] = {**schema, "description": param_desc}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒãªã‘ã‚Œã°å¿…é ˆ
        if param.default == inspect.Parameter.empty:
            required.append(param_name)

    # parameterså¼•æ•°ã«ã¯Schemaå‹ãŒå¿…è¦ã ãŒã€dictæ§‹é€ ã‚’æ¸¡ã™ã¨å†…éƒ¨ã§å¤‰æ›ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„
    # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ types.Schema(**...) ã§ãƒ©ãƒƒãƒ—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„
    # ç¾æ™‚ç‚¹ã§ã¯äº’æ›æ€§ã®ãŸã‚è¾æ›¸ã¨ã—ã¦æ¸¡ã™
    return types.FunctionDeclaration(
        name=tool_name,
        description=description,
        parameters={
            "type": "object",
            "properties": properties,
            "required": required,
        }
    )


class AgentRuntime:
    def __init__(
        self,
        config: AgentConfig,
        tool_map: Dict[str, Callable],
        agent_name: str = "agent",
        name: str = None,
        provider: str = None,
        model: str = None,
        stream: bool = True,
        verbose: bool = False,
        progress_callback: Optional[Callable] = None,
        parent_agent: Optional[str] = None,
        semantic_memory: Optional[SemanticMemory] = None,
        skills: Optional[List[SkillConfig]] = None
    ):
        self.config = config
        self.tool_map = tool_map
        self.agent_name = agent_name
        self.name = name or agent_name
        self.verbose = verbose
        self.progress_callback = progress_callback
        self.parent_agent = parent_agent
        self.skills: List[SkillConfig] = skills or []
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ«ãƒ¼ãƒ—æ¤œå‡º
        self.tool_tracker = ToolCallTracker(max_repeats=3, window_size=10)
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¡ãƒ¢ãƒªã®åˆæœŸåŒ–
        self.semantic_memory = semantic_memory
        if not self.semantic_memory:
            from pathlib import Path
            db_path = os.getenv("SEMANTIC_DB_PATH", str(Path.cwd() / "data" / "semantic.db"))
            self.semantic_memory = SemanticMemory(db_path=db_path)

        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æ±ºå®šï¼ˆç’°å¢ƒå¤‰æ•° or å¼•æ•° or è‡ªå‹•é¸æŠï¼‰
        from .llm_provider import get_available_provider
        if provider:
            self.provider = provider
        elif os.environ.get("LLM_PROVIDER"):
            self.provider = os.environ.get("LLM_PROVIDER")
        else:
            self.provider = get_available_provider()

        # ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š
        if model:
            self.model_name = model
        elif self.provider == LLMProvider.OPENROUTER:
            self.model_name = os.environ.get("OPENROUTER_MODEL", "google/gemini-3-flash-preview")  # reasoningå¯¾å¿œ
        elif self.provider == LLMProvider.OPENAI:
            self.model_name = os.environ.get("OPENAI_MODEL", "gpt-5.2-codex")
        elif self.provider == LLMProvider.ZAI:
            self.model_name = os.environ.get("ZAI_MODEL", "glm-4.7")
        else:
            self.model_name = os.environ.get("GEMINI_MODEL", "gemini-3-flash-preview")

        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.provider == LLMProvider.OPENROUTER:
            # OpenRouter ã¯ OpenAIäº’æ›API
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            openrouter_key = os.environ.get("OPENROUTER_API_KEY")
            if not openrouter_key:
                raise ValueError("OPENROUTER_API_KEY environment variable not set")
            self.openai_client = AsyncOpenAI(
                api_key=openrouter_key,
                base_url="https://openrouter.ai/api/v1"
            )
            self.client = None
        elif self.provider == LLMProvider.OPENAI:
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            openai_key = os.environ.get("OPENAI_API_KEY")
            if not openai_key:
                raise ValueError("OPENAI_API_KEY environment variable not set")
            self.openai_client = AsyncOpenAI(api_key=openai_key)
            self.client = None
        elif self.provider == LLMProvider.ZAI:
            # Z.ai GLM-4.7 (OpenAIäº’æ›API)
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAI package not installed. Run: pip install openai")
            zai_key = os.environ.get("ZAI_API_KEY")
            if not zai_key:
                raise ValueError("ZAI_API_KEY environment variable not set")
            self.openai_client = AsyncOpenAI(
                api_key=zai_key,
                base_url="https://api.z.ai/api/coding/paas/v4"
            )
            self.client = None
        else:
            # Gemini
            api_key = (
                os.environ.get("GENAI_API_KEY") or
                os.environ.get("GEMINI_API_KEY") or
                os.environ.get("GOOGLE_API_KEY")
            )
            if not api_key:
                raise ValueError("GENAI_API_KEY, GEMINI_API_KEY, or GOOGLE_API_KEY not set")
            self.client = genai.Client(api_key=api_key)
            self.openai_client = None

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¨­å®š
        self.stream = stream
        
        # éƒ¨åˆ†å¿œç­”ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ä¿å­˜ã™ã‚‹ãŸã‚ï¼‰
        self._partial_response = ""

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™ç®¡ç†ï¼ˆ1å›ã®run()å†…ã§ãƒªã‚»ãƒƒãƒˆï¼‰
        self._accumulated_tokens = 0
        self._tool_call_count = 0
        self._context_limit_reached = False

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ç”¨
        self.last_usage: Dict[str, Any] = {}

        # ãƒ„ãƒ¼ãƒ«ã®æº–å‚™
        self.available_tools = {}
        self.tool_declarations = []  # Geminiç”¨
        self.openai_tools = []       # OpenAIç”¨

        self._prepare_tools()

    def _prepare_tools(self):
        """æœ‰åŠ¹åŒ–ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’æº–å‚™ã™ã‚‹"""
        if not self.config.tools:
            return

        for tool_name in self.config.tools:
            if tool_name in self.tool_map:
                func = self.tool_map[tool_name]
                self.available_tools[tool_name] = func

                # Geminiç”¨ FunctionDeclaration
                declaration = _func_to_declaration(func, tool_name)
                self.tool_declarations.append(declaration)

                # OpenAIç”¨ toolå®šç¾©
                openai_tool = _func_to_openai_tool(func, tool_name)
                self.openai_tools.append(openai_tool)
            else:
                if self.verbose:
                    print(f"Warning: Tool '{tool_name}' not found in provided tool_map")

    def _update_context_usage(self, result: str) -> str:
        """
        ãƒ„ãƒ¼ãƒ«çµæœã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç´¯ç©ã—ã€ä¸Šé™ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã€‚
        è­¦å‘Šã¾ãŸã¯ä¸Šé™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ result ã«è¿½åŠ ã—ã¦è¿”ã™ã€‚
        """
        result_tokens = _estimate_tokens(result)
        self._accumulated_tokens += result_tokens
        self._tool_call_count += 1
        
        usage_ratio = self._accumulated_tokens / MAX_CONTEXT_TOKENS
        
        # 100%è¶…é: å¼·åˆ¶çµ‚äº†æŒ‡ç¤º
        if usage_ratio >= 1.0:
            self._context_limit_reached = True
            return (
                f"{result}\n\n"
                f"â›” **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™åˆ°é”** ({self._accumulated_tokens:,} / {MAX_CONTEXT_TOKENS:,} tokens)\n"
                f"ã“ã‚Œä»¥ä¸Šãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã•ãšã€ä»Šã‚ã‚‹æƒ…å ±ã§å¿œç­”ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å›æ•°ä¸Šé™ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ: ContextCompressor ã§ç®¡ç†
        # if self._tool_call_count >= MAX_TOOL_CALLS:
        #     self._context_limit_reached = True
        #     return (
        #         f"{result}\n\n"
        #         f"â›” **ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä¸Šé™åˆ°é”** ({self._tool_call_count} / {MAX_TOOL_CALLS} calls)\n"
        #         f"ã“ã‚Œä»¥ä¸Šãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã•ãšã€ä»Šã‚ã‚‹æƒ…å ±ã§å¿œç­”ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚"
        #     )
        
        # 80%è­¦å‘Š
        if usage_ratio >= CONTEXT_WARNING_THRESHOLD:
            remaining = MAX_CONTEXT_TOKENS - self._accumulated_tokens
            return (
                f"{result}\n\n"
                f"âš ï¸ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ {usage_ratio*100:.0f}% ä½¿ç”¨ä¸­ "
                f"(æ®‹ã‚Šç´„ {remaining:,} tokens)\n"
                f"å¿…è¦æœ€å°é™ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«æŠ‘ãˆã¦ãã ã•ã„ã€‚"
            )
        
        return result

    def _get_system_prompt(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã—ã€æ–‡è„ˆæƒ…å ±ã‚’æŒ¿å…¥ã™ã‚‹"""
        # JST (UTC+9)
        jst = timezone(timedelta(hours=9))
        now_dt = datetime.now(jst)
        now_str = now_dt.strftime("%Y-%m-%d %H:%M:%S (JST)")

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®æ§‹ç¯‰
        context_header = f"---\n[Current Context]\nTimestamp: {now_str}\n"
        
        # æƒ³èµ·çµæœã®è¿½åŠ 
        if hasattr(self, "_recall_results") and self._recall_results:
            context_header += "\n[Related Knowledge/Past Incidents]\n"
            for i, res in enumerate(self._recall_results):
                content = res.get('content', '')
                # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                if len(content) > 1000:
                    content = content[:1000] + "..."
                context_header += f"- Knowledge {i+1}:\n{content}\n"
        
        context_header += "---\n\n"

        prompt = self.config.system_prompt

        # Skills ã®æ³¨å…¥
        if self.skills:
            skills_section = "\n\n## Skills\n\n"
            for i, skill in enumerate(self.skills, 1):
                skills_section += f"### Skill: {skill.name} (v{skill.version})\n"
                skills_section += f"{skill.description}\n"
                if skill.allowed_tools:
                    skills_section += f"\n**Allowed Tools**: {', '.join(skill.allowed_tools)}\n"
                skills_section += f"\n{skill.content}\n\n"
            prompt += skills_section

        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç½®æ›
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨é‡è¤‡ã™ã‚‹å ´åˆãŒã‚ã‚‹ãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ç‰¹å®šä½ç½®ã«æ—¥æ™‚ã‚’åŸ‹ã‚è¾¼ã¿ãŸã„è¦æœ›ã«å¯¾å¿œ
        if "{{CURRENT_DATETIME}}" in prompt:
            prompt = prompt.replace("{{CURRENT_DATETIME}}", now_str)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆmdã§ {{SESSION_CONTEXT}} ã‚’ä½¿ã£ãŸå ´åˆã®ã¿æ³¨å…¥ï¼‰
        if "{{SESSION_CONTEXT}}" in prompt:
            session_context = self._build_session_context()
            prompt = prompt.replace("{{SESSION_CONTEXT}}", session_context)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆï¼ˆmdã§ {{AGENT_STATS}} ã‚’ä½¿ã£ãŸå ´åˆã®ã¿æ³¨å…¥ï¼‰
        if "{{AGENT_STATS}}" in prompt:
            agent_stats = self._build_agent_stats()
            prompt = prompt.replace("{{AGENT_STATS}}", agent_stats)

        # å…±é€šãƒ«ãƒ¼ãƒ«ã‚’æœ«å°¾ã«è¿½åŠ 
        return context_header + prompt + "\n\n" + COMMON_AGENT_RULES
    
    def _build_session_context(self) -> str:
        """ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        try:
            from ..tools.stats import get_session_stats
            return get_session_stats()
        except Exception as e:
            return f"(ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e})"
    
    def _build_agent_stats(self) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆã‚’æ§‹ç¯‰"""
        try:
            from ..tools.stats import get_agent_stats
            return get_agent_stats(days=7)
        except Exception as e:
            return f"(ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e})"

    async def _execute_tool_with_tracking(
        self, 
        func_name: str, 
        args_dict: Dict[str, Any], 
        session_id: Optional[str] = None
    ) -> str:
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã¨ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã®å…±é€šå‡¦ç†
        
        Args:
            func_name: ãƒ„ãƒ¼ãƒ«å
            args_dict: ãƒ„ãƒ¼ãƒ«å¼•æ•°
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
            
        Returns:
            ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ
        """
        # ãƒ­ã‚°å‡ºåŠ›
        _log_tool_use(func_name, args_dict, self.verbose)
        
        # é–‹å§‹é€šçŸ¥
        if self.progress_callback:
            icon, name, arg_str, _ = _format_tool_log(func_name, args_dict)
            self.progress_callback(
                event_type="tool", 
                name=f"{icon} {name}", 
                detail=arg_str, 
                agent_name=self.agent_name, 
                parent_agent=self.parent_agent, 
                status="running",
                tool_name=func_name
            )
        
        if self.verbose:
            print(f"  args: {args_dict}")

        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
        if session_id:
            check_cancelled(session_id)

        # ãƒ«ãƒ¼ãƒ—æ¤œå‡º
        allowed, block_msg = self.tool_tracker.check_and_record(func_name, args_dict)
        if not allowed:
            result = block_msg
        elif func_name in self.available_tools:
            try:
                raw_result = await _execute_tool_safely_async(self.available_tools[func_name], args_dict)
                result = _truncate_tool_output(raw_result, func_name)
            except Exception as e:
                result = f"Error executing {func_name}: {e}"
        else:
            result = f"Error: Tool {func_name} not found"

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™ãƒã‚§ãƒƒã‚¯
        result = self._update_context_usage(result)

        # çµ‚äº†é€šçŸ¥
        if self.progress_callback:
            icon, name, arg_str, _ = _format_tool_log(func_name, args_dict)
            self.progress_callback(
                event_type="tool", 
                name=f"{icon} {name}", 
                detail=arg_str, 
                agent_name=self.agent_name, 
                parent_agent=self.parent_agent, 
                status="completed",
                tool_name=func_name,
                result=result
            )

        return result

    async def run(self, user_input: str, history: Optional[List[Any]] = None, session_id: Optional[str] = None) -> str:
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹
        """
        if self.progress_callback:
            self.progress_callback(event_type="start", agent_name=self.name)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ä¿å­˜
        self._current_session_id = session_id

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™ç®¡ç†ã‚’ãƒªã‚»ãƒƒãƒˆ
        self._accumulated_tokens = 0
        self._tool_call_count = 0
        self._context_limit_reached = False
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ«ãƒ¼ãƒ—æ¤œå‡ºã‚’ãƒªã‚»ãƒƒãƒˆ
        self.tool_tracker.reset()

        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
        if session_id:
            check_cancelled(session_id)

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®æƒ³èµ·
        self._recall_results = []
        try:
            self._recall_results = self.semantic_memory.search(user_input, top_k=3)
            if self._recall_results and self.progress_callback:
                self.progress_callback(
                    event_type="recall",
                    agent_name=self.name,
                    results=self._recall_results
                )
        except Exception as e:
            if self.verbose:
                print(f"Warning: Semantic recall failed: {e}")

        if self.provider in (LLMProvider.OPENAI, LLMProvider.OPENROUTER, LLMProvider.ZAI):
            result = await self._run_openai(user_input, history, session_id=session_id)
        else:
            result = await self._run_gemini(user_input, history, session_id=session_id)

        # CostTrackerã«ã‚³ã‚¹ãƒˆã‚’è¨˜éŒ²
        self._record_cost()

        if self.progress_callback:
            self.progress_callback(event_type="done", status="completed", agent_name=self.name)

        return result

    def _record_cost(self) -> None:
        """CostTrackerã«ã‚³ã‚¹ãƒˆã‚’è¨˜éŒ²"""
        if not self.last_usage:
            return
        try:
            from moco.core.cost_tracker import get_cost_tracker, TokenUsage
            from moco.storage.usage_store import get_usage_store
            
            tracker = get_cost_tracker()
            usage_store = get_usage_store()
            
            usage = TokenUsage(
                input_tokens=self.last_usage.get("prompt_tokens", 0),
                output_tokens=self.last_usage.get("completion_tokens", 0),
            )
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€åã‚’æ±ºå®š
            provider_name = "gemini" if self.provider == LLMProvider.GEMINI else "openai"
            if self.provider == LLMProvider.OPENROUTER:
                provider_name = "openrouter"
            elif self.provider == LLMProvider.ZAI:
                provider_name = "zai"

            session_id = getattr(self, "_current_session_id", None)

            record = tracker.record(
                provider=provider_name,
                model=self.model_name,
                usage=usage,
                agent_name=self.name,
                session_id=session_id,
            )
            
            # DBã«ã‚‚æ°¸ç¶šåŒ–
            usage_store.record_usage(
                provider=provider_name,
                model=self.model_name,
                input_tokens=usage.input_tokens,
                output_tokens=usage.output_tokens,
                cost_usd=record.cost_usd,
                session_id=session_id,
                agent_name=self.name
            )
        except Exception as e:
            if self.verbose:
                print(f"Warning: Failed to record cost: {e}")

    def get_metrics(self) -> Dict[str, Any]:
        """æœ€æ–°ã®å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        return self.last_usage

    async def _run_openai(self, user_input: str, history: Optional[List[Any]] = None, session_id: Optional[str] = None) -> str:
        """OpenAI GPTã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
        if history is None:
            history = []

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        messages = [{"role": "system", "content": self._get_system_prompt()}]

        # å±¥æ­´ã‚’è¿½åŠ 
        for h in history:
            if isinstance(h, dict):
                role = h.get("role", "user")
                # Geminiå½¢å¼ â†’ OpenAIå½¢å¼ã«å¤‰æ›
                if role == "model":
                    role = "assistant"
                # tool/function ãƒ­ãƒ¼ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã§æœªã‚µãƒãƒ¼ãƒˆï¼‰
                if role not in ("system", "user", "assistant"):
                    continue
                content = h.get("content", "")
                if not content and "parts" in h:
                    parts = h.get("parts", [])
                    content = " ".join(str(p) for p in parts)
                if content:  # ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—
                    messages.append({"role": role, "content": content})

        messages.append({"role": "user", "content": user_input})

        # ãƒ„ãƒ¼ãƒ«è¨­å®š
        tools = self.openai_tools if self.openai_tools else None

        # max_iterations ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ: ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã§ç®¡ç†
        # iterations = 0
        # max_iterations = 20
        while True:
            if session_id:
                check_cancelled(session_id)
            
            # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è­¦å‘Šã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ã§ç®¡ç†ï¼‰
            # remaining = max_iterations - iterations
            # if remaining <= 3 and remaining > 0:
            #     warning_msg = ...
            
            try:
                if self.stream:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                    # reasoning/thinking ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    if _is_reasoning_model(self.model_name):
                        extra_body = {}
                        
                        # OpenRouter ã®å ´åˆã¯ reasoning ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                        # effort ã¨ max_tokens ã¯åŒæ™‚ã«æŒ‡å®šã§ããªã„
                        if self.provider == LLMProvider.OPENROUTER:
                            extra_body["reasoning"] = {
                                "effort": "high"  # low, medium, high, xhigh
                            }
                        
                        create_kwargs = {
                            "model": self.model_name,
                            "messages": messages,
                            "tools": tools,
                            "stream": True,
                            "stream_options": {"include_usage": True},
                            "parallel_tool_calls": True,
                        }
                        
                        # OpenAI o1/o3 ã®å ´åˆã¯ reasoning_effort ã‚’ä½¿ç”¨
                        if self.provider != LLMProvider.OPENROUTER:
                            create_kwargs["reasoning_effort"] = "medium"
                        
                        if extra_body:
                            create_kwargs["extra_body"] = extra_body
                        
                        response = await self.openai_client.chat.completions.create(**create_kwargs)
                    else:
                        response = await self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            tools=tools,
                            temperature=0.7,
                            stream=True,
                            stream_options={"include_usage": True},
                            parallel_tool_calls=True,
                        )

                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
                    collected_content = ""
                    collected_tool_calls = []
                    current_tool_call = None
                    # æ€è€ƒãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ï¼ˆGLMç­‰ã®ç´°ã‹ã„ãƒãƒ£ãƒ³ã‚¯å¯¾ç­–ï¼‰
                    reasoning_buffer = ""
                    reasoning_header_shown = False

                    async for chunk in response:
                        # usageæƒ…å ±ã®å–å¾—ï¼ˆæœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã«å«ã¾ã‚Œã‚‹ï¼‰
                        if hasattr(chunk, "usage") and chunk.usage:
                            self.last_usage = {
                                "prompt_tokens": chunk.usage.prompt_tokens,
                                "completion_tokens": chunk.usage.completion_tokens,
                                "total_tokens": chunk.usage.total_tokens
                            }

                        delta = chunk.choices[0].delta if chunk.choices else None
                        if not delta:
                            continue

                        # reasoning/thinking ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†
                        # OpenRouter: delta.reasoning ã¾ãŸã¯ delta.reasoning_details
                        # OpenAI o1/o3: delta.reasoning_content
                        reasoning_text = None
                        # OpenRouter: reasoning ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆæ–‡å­—åˆ—ï¼‰
                        if hasattr(delta, 'reasoning') and delta.reasoning:
                            reasoning_text = delta.reasoning
                        # OpenRouter: reasoning_details ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆé…åˆ—ï¼‰
                        elif hasattr(delta, 'reasoning_details') and delta.reasoning_details:
                            if isinstance(delta.reasoning_details, list):
                                for detail in delta.reasoning_details:
                                    # dict ã®å ´åˆ
                                    if isinstance(detail, dict) and detail.get('text'):
                                        reasoning_text = detail['text']
                                        break
                                    # object ã®å ´åˆ
                                    elif hasattr(detail, 'text') and detail.text:
                                        reasoning_text = detail.text
                                        break
                            else:
                                reasoning_text = str(delta.reasoning_details)
                        # OpenAI o1/o3: reasoning_content
                        elif hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                            reasoning_text = delta.reasoning_content
                        
                        if reasoning_text:
                            if self.progress_callback:
                                # Web UIçµŒç”±: progress_callback ã§ãƒãƒƒãƒåŒ–ã—ã¦é€ä¿¡
                                self.progress_callback(
                                    event_type="thinking",
                                    content=reasoning_text,
                                    agent_name=self.name
                                )
                            else:
                                # CLIç›´æ¥å®Ÿè¡Œ: verbose ã®ã¨ãã ã‘æ€è€ƒéç¨‹ã‚’è¡¨ç¤ºã™ã‚‹
                                if self.verbose and not self.progress_callback:
                                    # ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã—ã¦å¥ç‚¹/æ”¹è¡Œ/ä¸€å®šæ–‡å­—æ•°ã§ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                                    reasoning_buffer += reasoning_text
                                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’1å›ã ã‘è¡¨ç¤º
                                    if not reasoning_header_shown:
                                        _safe_stream_print("\nğŸ’­ [æ€è€ƒä¸­...]\n")
                                        reasoning_header_shown = True
                                    # ãƒ•ãƒ©ãƒƒã‚·ãƒ¥æ¡ä»¶: å¥ç‚¹ã€æ”¹è¡Œã€ã¾ãŸã¯80æ–‡å­—ä»¥ä¸Š
                                    while len(reasoning_buffer) >= 80 or any(c in reasoning_buffer for c in 'ã€‚\n'):
                                        # å¥ç‚¹ã‹æ”¹è¡ŒãŒã‚ã‚Œã°ãã“ã¾ã§å‡ºåŠ›
                                        flush_pos = -1
                                        for i, c in enumerate(reasoning_buffer):
                                            if c in 'ã€‚\n':
                                                flush_pos = i + 1
                                                break
                                        if flush_pos == -1 and len(reasoning_buffer) >= 80:
                                            flush_pos = 80
                                        if flush_pos > 0:
                                            _safe_stream_print(reasoning_buffer[:flush_pos])
                                            reasoning_buffer = reasoning_buffer[flush_pos:]
                                        else:
                                            break
                                else:
                                    # verbose ã§ãªã„å ´åˆã¯æ€è€ƒãƒãƒƒãƒ•ã‚¡ã‚’ä½¿ã‚ãªã„
                                    reasoning_buffer = ""
                        # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›
                        if delta.content:
                            if not self.progress_callback:
                                _safe_stream_print(delta.content)
                            collected_content += delta.content
                            self._partial_response = collected_content  # ã‚¨ãƒ©ãƒ¼æ™‚ã®å¾©æ—§ç”¨
                            if self.progress_callback:
                                self.progress_callback(
                                    event_type="chunk",
                                    content=delta.content,
                                    agent_name=self.name
                                )

                        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®åé›†
                        if delta.tool_calls:
                            for tc_delta in delta.tool_calls:
                                if tc_delta.index is not None:
                                    # æ–°ã—ã„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã¾ãŸã¯æ—¢å­˜ã®æ›´æ–°
                                    while len(collected_tool_calls) <= tc_delta.index:
                                        collected_tool_calls.append({
                                            "id": "",
                                            "type": "function",
                                            "function": {"name": "", "arguments": ""}
                                        })

                                    tc = collected_tool_calls[tc_delta.index]
                                    if tc_delta.id:
                                        tc["id"] = tc_delta.id
                                    if tc_delta.function:
                                        if tc_delta.function.name:
                                            tc["function"]["name"] = tc_delta.function.name
                                        if tc_delta.function.arguments:
                                            tc["function"]["arguments"] += tc_delta.function.arguments

                    # æ®‹ã‚Šã®æ€è€ƒãƒãƒƒãƒ•ã‚¡ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆverbose ã®ã¨ãã ã‘ï¼‰
                    if reasoning_buffer and self.verbose and not self.progress_callback:
                        _safe_stream_print(reasoning_buffer)
                        reasoning_buffer = ""
                    if reasoning_header_shown and self.verbose and not self.progress_callback:
                        _safe_stream_print("\n[/æ€è€ƒ]\n")

                    if collected_content and not self.progress_callback:
                        _safe_stream_print("\n")  # æ”¹è¡Œ

                    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ç¢ºèª
                    if collected_tool_calls and collected_tool_calls[0]["id"]:
                        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å‡¦ç†
                        messages.append({
                            "role": "assistant",
                            "content": collected_content or "",
                            "tool_calls": collected_tool_calls
                        })

                        for tc in collected_tool_calls:
                            func_name = tc["function"]["name"]
                            try:
                                args_dict = json.loads(tc["function"]["arguments"])
                            except json.JSONDecodeError:
                                args_dict = {}

                            result = await self._execute_tool_with_tracking(func_name, args_dict, session_id)

                            messages.append({
                                "role": "tool",
                                "tool_call_id": tc["id"],
                                "content": str(result)
                            })
                        
                        # 80%è¶…éæ™‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®
                        usage_ratio = self._accumulated_tokens / MAX_CONTEXT_TOKENS
                        if usage_ratio >= CONTEXT_WARNING_THRESHOLD:
                            compressor = ContextCompressor(
                                max_tokens=int(MAX_CONTEXT_TOKENS * CONTEXT_WARNING_THRESHOLD),
                                preserve_recent=10
                            )
                            messages, was_compressed = compressor.compress_if_needed(messages, self.provider)
                            if was_compressed:
                                self._accumulated_tokens = compressor.estimate_tokens(messages)
                                if self.verbose:
                                    print(f"\nğŸ—œï¸ [Context compressed: {self._accumulated_tokens:,} tokens]")
                        
                        continue  # æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                    else:
                        # ç©ºã®å ´åˆã¯éƒ¨åˆ†å¿œç­”ã‚’è¿”ã™
                        if not collected_content and self._partial_response:
                            return self._partial_response
                        return collected_content
                else:
                    # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                    # reasoning ãƒ¢ãƒ‡ãƒ« (o1/o3) ã®å ´åˆã¯ reasoning_effort ã‚’ä½¿ç”¨
                    if _is_reasoning_model(self.model_name):
                        response = await self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            tools=tools,
                            reasoning_effort="medium",  # thinking mode ON
                            parallel_tool_calls=True,
                        )
                    else:
                        response = await self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            tools=tools,
                            temperature=0.7,
                            parallel_tool_calls=True,
                        )
                    # usageè¨˜éŒ²
                    if hasattr(response, "usage") and response.usage:
                        self.last_usage = {
                            "prompt_tokens": response.usage.prompt_tokens,
                            "completion_tokens": response.usage.completion_tokens,
                            "total_tokens": response.usage.total_tokens
                        }
            except Exception as e:
                return f"Error calling OpenAI API: {e}"

            if not self.stream:
                choice = response.choices[0]
                message = choice.message
            else:
                continue  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆã¯ä¸Šã§å‡¦ç†æ¸ˆã¿

            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®ç¢ºèªï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
            if message.tool_calls:
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                messages.append({
                    "role": "assistant",
                    "content": message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in message.tool_calls
                    ]
                })

                # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
                for tc in message.tool_calls:
                    func_name = tc.function.name
                    try:
                        args_dict = json.loads(tc.function.arguments)
                    except json.JSONDecodeError:
                        args_dict = {}

                    result = await self._execute_tool_with_tracking(func_name, args_dict, session_id)

                    # ãƒ„ãƒ¼ãƒ«çµæœã‚’è¿½åŠ 
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": str(result)
                    })
                
                # 80%è¶…éæ™‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåœ§ç¸®
                usage_ratio = self._accumulated_tokens / MAX_CONTEXT_TOKENS
                if usage_ratio >= CONTEXT_WARNING_THRESHOLD:
                    compressor = ContextCompressor(
                        max_tokens=int(MAX_CONTEXT_TOKENS * CONTEXT_WARNING_THRESHOLD),
                        preserve_recent=10
                    )
                    messages, was_compressed = compressor.compress_if_needed(messages, self.provider)
                    if was_compressed:
                        self._accumulated_tokens = compressor.estimate_tokens(messages)
                        if self.verbose:
                            print(f"[Context compressed: {self._accumulated_tokens:,} tokens]")
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã‚’è¿”ã™
                return message.content or ""

        # max_iterations ã«é”ã—ãŸå ´åˆ
        if self._partial_response:
            return f"[æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ°é”]\n{self._partial_response}"
        return "Error: Max iterations reached without response."

    async def _run_gemini(self, user_input: str, history: Optional[List[Any]] = None, session_id: Optional[str] = None) -> str:
        """Geminiã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ"""
        if history is None:
            history = []

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        system_instruction = self._get_system_prompt()

        # ãƒ„ãƒ¼ãƒ«è¨­å®š
        tools_config = None
        if self.tool_declarations:
            # Pythoné–¢æ•°ã‚’ãã®ã¾ã¾æ¸¡ã™ã¨è‡ªå‹•çš„ã«ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆã•ã‚Œã‚‹
            tools_config = [types.Tool(function_declarations=self.tool_declarations)]

        messages = []
        # å±¥æ­´ã®è¿½åŠ ï¼ˆdictå½¢å¼ã‚’Contentå½¢å¼ã«å¤‰æ›ï¼‰
        for h in history:
            if isinstance(h, dict):
                role = h.get("role", "user")
                parts = h.get("parts", [])
                if not parts and "content" in h:
                    parts = [h["content"]]
                # partsã‚’Partã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                part_objects = []
                for p in parts:
                    if isinstance(p, str):
                        part_objects.append(types.Part(text=p))
                    else:
                        part_objects.append(p)
                messages.append(types.Content(role=role, parts=part_objects))
            else:
                # æ—¢ã«Contentå½¢å¼ã®å ´åˆ
                messages.append(h)

        messages.append(types.Content(role="user", parts=[types.Part(text=user_input)]))

        # è¨­å®šï¼ˆthinking mode ã‚’æœ‰åŠ¹åŒ–ï¼‰
        config = types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=tools_config,
            temperature=0.7,
            thinking_config=types.ThinkingConfig(
                include_thoughts=True,
                thinking_budget=32768,  # APIä»•æ§˜ä¸Šã®æœ€å°å€¤
            ),
        )

        while True:
            if session_id:
                check_cancelled(session_id)

            try:
                if self.stream:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                    response_stream = self.client.models.generate_content_stream(
                        model=self.model_name,
                        contents=messages,
                        config=config
                    )

                    collected_text = ""
                    collected_parts = []
                    function_calls = []

                    for chunk in response_stream:
                        # usageæƒ…å ±ã®å–å¾—
                        if chunk.usage_metadata:
                            self.last_usage = {
                                "prompt_tokens": chunk.usage_metadata.prompt_token_count,
                                "completion_tokens": chunk.usage_metadata.candidates_token_count,
                                "total_tokens": chunk.usage_metadata.total_token_count
                            }

                        if not chunk.candidates:
                            continue

                        candidate = chunk.candidates[0]
                        if not candidate.content:
                            continue

                        for part in candidate.content.parts or []:
                            # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è¡¨ç¤º (verbose ãƒ¢ãƒ¼ãƒ‰ã®ã¿)
                            if hasattr(part, 'thought') and part.thought and part.text:
                                if self.progress_callback:
                                    self.progress_callback(
                                        event_type="thinking",
                                        content=part.text,
                                        agent_name=self.name
                                    )
                                elif self.verbose:
                                    thought_text = f"\nğŸ’­ [æ€è€ƒä¸­...]\n{part.text}\n[/æ€è€ƒ]\n"
                                    _safe_stream_print(thought_text)
                                continue
                            if part.text:
                                if not self.progress_callback:
                                    _safe_stream_print(part.text)
                                collected_text += part.text
                                self._partial_response = collected_text
                                collected_parts.append(part)
                                if self.progress_callback:
                                    self.progress_callback(
                                        event_type="chunk",
                                        content=part.text,
                                        agent_name=self.name
                                    )
                            if part.function_call:
                                function_calls.append(part.function_call)
                                collected_parts.append(part)

                    if collected_text and not self.progress_callback:
                        _safe_stream_print("\n")

                    if function_calls:
                        messages.append(types.Content(role="model", parts=collected_parts))
                        tool_responses = []
                        for fc in function_calls:
                            func_name = fc.name
                            args = fc.args
                            args_dict = {}
                            if args:
                                if hasattr(args, "items"):
                                    args_dict = {k: v for k, v in args.items()}
                                elif isinstance(args, dict):
                                    args_dict = args

                            result = await self._execute_tool_with_tracking(func_name, args_dict, session_id)

                            tool_responses.append(
                                types.Part(
                                    function_response=types.FunctionResponse(
                                        name=func_name,
                                        response={"result": _ensure_jsonable(result)}
                                    )
                                )
                            )
                        messages.append(types.Content(role="tool", parts=tool_responses))
                        continue
                    else:
                        return collected_text

                else:
                    # éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
                    response = self.client.models.generate_content(
                        model=self.model_name,
                        contents=messages,
                        config=config
                    )
                    
                    if response.usage_metadata:
                        self.last_usage = {
                            "prompt_tokens": response.usage_metadata.prompt_token_count,
                            "completion_tokens": response.usage_metadata.candidates_token_count,
                            "total_tokens": response.usage_metadata.total_token_count
                        }

                    if not response.candidates:
                        return "Error: No response candidates from Gemini."

                    candidate = response.candidates[0]
                    message = candidate.content
                    if not message:
                        return "Error: Empty response content from Gemini."

                    messages.append(message)
                    
                    for part in message.parts or []:
                        if hasattr(part, 'thought') and part.thought and part.text:
                            if self.progress_callback:
                                self.progress_callback(
                                    event_type="thinking",
                                    content=part.text,
                                    agent_name=self.name
                                )
                            elif self.verbose:
                                print(f"\nğŸ’­ [æ€è€ƒä¸­...]\n{part.text}\n[/æ€è€ƒ]")

                    function_calls = [p.function_call for p in message.parts if p.function_call]
                    if function_calls:
                        tool_responses = []
                        for fc in function_calls:
                            func_name = fc.name
                            args = fc.args
                            args_dict = {}
                            if args:
                                if hasattr(args, "items"):
                                    args_dict = {k: v for k, v in args.items()}
                                elif isinstance(args, dict):
                                    args_dict = args

                            result = await self._execute_tool_with_tracking(func_name, args_dict, session_id)

                            tool_responses.append(
                                types.Part(
                                    function_response=types.FunctionResponse(
                                        name=func_name,
                                        response={"result": _ensure_jsonable(result)}
                                    )
                                )
                            )
                        messages.append(types.Content(role="tool", parts=tool_responses))
                        continue
                    else:
                        full_text = "".join(p.text for p in message.parts if p.text)
                        return full_text

            except Exception as e:
                import traceback
                traceback.print_exc()
                return f"Error calling Gemini API: {e}"
