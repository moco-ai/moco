"""
Memory Service - è¨˜æ†¶ãƒ»å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  API (SQLiteç‰ˆ)
æ—¢å­˜ã®LLM Botã«å·®ã—è¾¼ã‚“ã§ä½¿ç”¨ã™ã‚‹

ä½¿ã„æ–¹:
    from ai_manager.core.memory_service import MemoryService
    
    memory = MemoryService(channel_id="C01234567")
    
    # 1. ä¼šè©±å‰: é–¢é€£è¨˜æ†¶ã‚’å–å¾—
    memories = memory.recall("çµŒè²»ç²¾ç®—ã©ã“ï¼Ÿ")
    
    # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¨˜æ†¶ã‚’è¿½åŠ 
    prompt = f"ã€è¨˜æ†¶ã€‘{memories}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_message}"
    response = llm.generate(prompt)
    
    # 3. ä¼šè©±å¾Œ: å­¦ç¿’ã™ã¹ãã‹åˆ†æ
    result = memory.analyze(user_message, response)
    
    # 4. å¿…è¦ãªã‚‰è¨˜æ†¶ä¿å­˜

Note: LLM ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã® JSON è§£æã«ã¯ SmartJSONParser ã‚’ä½¿ç”¨
    if result["should_learn"]:
        memory.learn(result)
"""

import os
import re
import json
from typing import List, Dict, Optional, Tuple, Any, Iterator
from datetime import datetime
from dotenv import load_dotenv

from .db import init_db, get_conn
from .embeddings import build_genai_client, embed_text
from .serialization import serialize_embedding, deserialize_embedding, deserialize_keywords
from .similarity import cos_sim
from ..utils.json_parser import SmartJSONParser

# Lazy import for GraphStore (requires networkx)
GraphStore = None

load_dotenv()

class MemoryService:
    """
    è¨˜æ†¶ãƒ»å­¦ç¿’ã‚µãƒ¼ãƒ“ã‚¹ (SQLiteç‰ˆ)
    
    æ©Ÿèƒ½:
    - recall: é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢: embedding + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
    - analyze: ä¼šè©±ã‚’åˆ†æã—ã¦å­¦ç¿’ã™ã¹ãã‹åˆ¤å®š
    - learn: è¨˜æ†¶ã‚’ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    
    ãƒãƒ£ãƒ³ãƒãƒ«åˆ†é›¢:
    - channel_id ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã¯1ã¤ã®DBã«ä¿å­˜
    """
    
    def __init__(
        self,
        channel_id: Optional[str] = None,
        router_id: Optional[str] = None,
        worker_id: Optional[str] = None,
        db_path: str = "memories/memory.db",
        embedding_model: str = "gemini-embedding-001",
        feedback_threshold: int = 7,
        duplicate_threshold: float = 0.85,
        graph_enabled: bool = True
    ):
        self.channel_id = channel_id
        self.router_id = router_id or ''
        self.worker_id = worker_id or ''
        self.db_path = db_path
        self.embedding_model = embedding_model
        self.feedback_threshold = feedback_threshold
        self.duplicate_threshold = duplicate_threshold
        self.graph_enabled = graph_enabled
        
        # DBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(os.path.dirname(db_path) or ".", exist_ok=True)
        
        # DBåˆæœŸåŒ–
        self._init_db()
        
        # Embedding client (optional)
        self.genai_client = build_genai_client()

        # Graph Store (optional)
        self.graph = None
        if self.graph_enabled:
            try:
                from .graph import GraphStore as GS
                self.graph = GS(self.db_path)
            except ImportError:
                print("âš ï¸ GraphStore requires networkx. Install with: pip install networkx")
            except Exception as e:
                print(f"âš ï¸ Failed to initialize GraphStore: {e}")
    
    def _init_db(self):
        """DBãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–"""
        init_db(self.db_path)
    
    def _get_conn(self):
        """DBæ¥ç¶šã‚’å–å¾—"""
        return get_conn(self.db_path)
    
    def _embed(self, text: str) -> List[float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’embeddingã«å¤‰æ›"""
        return embed_text(self.genai_client, self.embedding_model, text)
    
    def _serialize_embedding(self, emb: List[float]) -> str:
        """embeddingã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›"""
        return serialize_embedding(emb)
    
    def _deserialize_embedding(self, emb_str: str) -> List[float]:
        """JSONæ–‡å­—åˆ—ã‹ã‚‰embeddingã‚’å¾©å…ƒ"""
        return deserialize_embedding(emb_str)
    
    def _deserialize_keywords(self, kw_str: str) -> List[str]:
        """JSONæ–‡å­—åˆ—ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å¾©å…ƒ"""
        return deserialize_keywords(kw_str)
    
    def _fetch_memories(
        self,
        exclude_types: Optional[List[str]] = None,
        limit: Optional[int] = None,
        since_days: Optional[int] = None,
        keyword_search: Optional[str] = None
    ) -> Iterator[Dict]:
        """
        è¨˜æ†¶ã‚’æ¡ä»¶ä»˜ãã§å–å¾—ï¼ˆGeneratorï¼‰
        """
        conn = self._get_conn()
        try:
            cursor = conn.cursor()
            
            query = "SELECT id, content, type, keywords, source, embedding, created_at, channel_id, questions FROM memories WHERE 1=1"
            params = []
            
            # Channel filter
            if self.channel_id:
                query += " AND (channel_id = ? OR channel_id = 'GLOBAL')"
                params.append(self.channel_id)
            
            if self.router_id:
                query += ' AND router_id = ?'
                params.append(self.router_id)
            if self.worker_id:
                query += ' AND worker_id = ?'
                params.append(self.worker_id)
            
            # Type filter
            if exclude_types:
                placeholders = ",".join(["?"] * len(exclude_types))
                query += f" AND type NOT IN ({placeholders})"
                params.extend(exclude_types)
                
            # Time filter
            if since_days:
                query += " AND created_at > datetime('now', ?)"
                params.append(f"-{since_days} days")
                
            # Keyword search (Rough filter via LIKE)
            if keyword_search:
                # ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã—ã¦æ¤œç´¢
                tokens = re.findall(r'[a-zA-Z0-9]{2,}|[\u4e00-\u9fff]+|[\u3040-\u309f]{2,}|[\u30a0-\u30ff]{2,}', keyword_search)
                if tokens:
                    token_queries = []
                    for t in tokens[:5]: # æœ€å¤§5ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§
                        token_queries.append("(content LIKE ? OR keywords LIKE ? OR questions LIKE ?)")
                        params.append(f"%{t}%")
                        params.append(f"%{t}%")
                        params.append(f"%{t}%")
                    query += " AND (" + " OR ".join(token_queries) + ")"


            # Order by created_at DESC
            query += " ORDER BY created_at DESC"
            
            if limit:
                query += " LIMIT ?"
                params.append(limit)
                
            cursor.execute(query, tuple(params))
            
            while True:
                row = cursor.fetchone()
                if not row:
                    break
                yield {
                    "id": row[0],
                    "content": row[1],
                    "type": row[2],
                    "keywords": self._deserialize_keywords(row[3]),
                    "source": row[4],
                    "emb": self._deserialize_embedding(row[5]),
                    "created_at": row[6],
                    "channel_id": row[7],
                    "questions": json.loads(row[8] or "[]") if isinstance(row[8], str) else (row[8] or [])
                }
        finally:
            conn.close()


    def _get_all_memories(self) -> List[Dict]:
        """
        å…¨è¨˜æ†¶ã‚’å–å¾— (äº’æ›æ€§ç¶­æŒã®ãŸã‚)
        """
        return list(self._fetch_memories())
    
    def _is_duplicate(self, text: str) -> Tuple[bool, Optional[str]]:
        """é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        # ç›´è¿‘100ä»¶ç¨‹åº¦ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯ã™ã‚Œã°ååˆ† (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚)
        memory_gen = self._fetch_memories(limit=100)
        
        new_emb = self._embed(text)
        if not new_emb:
            return False, None
        
        for m in memory_gen:
            if m['emb']:
                sim = cos_sim(new_emb, m['emb'])
                if sim >= self.duplicate_threshold:
                    return True, m['content']
        return False, None
    
    def recall(
        self,
        query: str,
        top_k: int = 10,
        exclude_types: Optional[List[str]] = None,
        since_days: Optional[int] = None
    ) -> List[Dict]:
        """
        é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: è¿”ã™è¨˜æ†¶ã®æ•° (Default: 10)
            exclude_types: é™¤å¤–ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
            since_days: éå»ä½•æ—¥åˆ†ã®è¨˜æ†¶ã‚’å¯¾è±¡ã«ã™ã‚‹ã‹ (Noneã®å ´åˆã¯å…¨æœŸé–“ã€ãŸã ã—æœ€å¤§1000ä»¶)
        
        Returns:
            é–¢é€£è¨˜æ†¶ã®ãƒªã‚¹ãƒˆ
        """
        if exclude_types is None:
            exclude_types = []
        
        # SQLãƒ¬ãƒ™ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        # 1. æœ€è¿‘ã®è¨˜æ†¶ã‚’å–å¾— (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)
        memory_gen_recent = self._fetch_memories(
            exclude_types=exclude_types,
            limit=500,
            since_days=since_days
        )
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒãƒƒãƒã™ã‚‹è¨˜æ†¶ã‚’åºƒã‚ã«å–å¾— (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒç”¨)
        memory_gen_kw = self._fetch_memories(
            exclude_types=exclude_types,
            limit=500,
            keyword_search=query,
            since_days=since_days
        )
        
        # Generatorã‚’çµ±åˆã—ã¦é‡è¤‡æ’é™¤
        seen_ids = set()
        def combined_gen():
            for m in memory_gen_recent:
                if m['id'] not in seen_ids:
                    seen_ids.add(m['id'])
                    yield m
            for m in memory_gen_kw:
                if m['id'] not in seen_ids:
                    seen_ids.add(m['id'])
                    yield m

        query_emb = self._embed(query)
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã®n-gramé›†åˆã‚’äº‹å‰ä½œæˆ (2-gram)
        query_ngrams = set()
        for i in range(len(query) - 1):
            query_ngrams.add(query[i:i+2])
        
        # ã‚°ãƒ©ãƒ•æ¤œç´¢ (ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹)
        graph_boosts = {}
        if self.graph:
            # ã‚¯ã‚¨ãƒªã‹ã‚‰å˜èªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ï¼‰
            words = re.findall(r'[a-zA-Z0-9]{2,}|[\u4e00-\u9fff]+', query)
            for word in words:
                related = self.graph.get_related(word, max_hops=1)
                for rel in related:
                    mid = rel.get("memory_id")
                    if mid:
                        graph_boosts[mid] = graph_boosts.get(mid, 0) + 0.2

        results = []
        now = datetime.now()
        
        for m in combined_gen():
            # é™¤å¤–ã‚¿ã‚¤ãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ— (å¿µã®ãŸã‚å†åº¦ãƒã‚§ãƒƒã‚¯)
            mem_type = m.get('type', 'unknown')
            if mem_type in exclude_types:
                continue
            
            # Embeddingã‚¹ã‚³ã‚¢
            emb_score = 0
            if query_emb and m['emb']:
                emb_score = max(0, cos_sim(query_emb, m['emb']))
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã‚¹ã‚³ã‚¢ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
            kw_list = m.get('keywords', [])
            qs_list = m.get('questions', [])
            all_match_targets = kw_list + qs_list
            
            matched = 0
            for kw in all_match_targets:
                hit = False
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®2æ–‡å­—ãŒã‚¯ã‚¨ãƒªã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                for i in range(len(kw) - 1):
                    if kw[i:i+2] in query_ngrams:
                        hit = True
                        break
                if not hit:
                    # ã‚¯ã‚¨ãƒªã®2æ–‡å­—ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    for i in range(len(query) - 1):
                        if query[i:i+2] in kw:
                            hit = True
                            break
                if hit:
                    matched += 1
            kw_score = matched / len(all_match_targets) if all_match_targets else 0

            
            # Recency Score (ç›´è¿‘æ€§) - é«˜é€ŸåŒ–ç‰ˆ
            recency_score = 0
            created_at_val = m.get("created_at")
            if created_at_val:
                try:
                    # SQLite timestamp format: YYYY-MM-DD HH:MM:SS
                    # æ–‡å­—åˆ—ã®å…ˆé ­10æ–‡å­—(YYYY-MM-DD)ã ã‘ã§åˆ¤å®š
                    created_date_str = str(created_at_val)[:10]
                    created_date = datetime.strptime(created_date_str, "%Y-%m-%d")
                    days_diff = (now - created_date).days
                    if days_diff < 30:
                        recency_score = 0.15 * (1 - (max(0, days_diff) / 30))
                except Exception:
                    pass

            # Success Bonus (æˆåŠŸä½“é¨“ãƒœãƒ¼ãƒŠã‚¹)
            success_bonus = 0
            if "Success: True" in m['content']:
                success_bonus = 0.1
            
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ + è£œæ­£
            hybrid_score = (emb_score * 0.5) + (kw_score * 0.4) + recency_score + success_bonus
            
            # ã‚°ãƒ©ãƒ•ãƒ–ãƒ¼ã‚¹ãƒˆ
            if m['id'] in graph_boosts:
                hybrid_score += graph_boosts[m['id']]

            if hybrid_score > 0.1:  # æœ€ä½ã—ãã„å€¤
                results.append({
                    "id": m['id'],
                    "content": m['content'],
                    "type": m.get('type', 'unknown'),
                    "score": hybrid_score
                })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def analyze(
        self,
        user_message: str,
        bot_response: Optional[str] = None,
        llm_client = None
    ) -> Dict:
        """
        ä¼šè©±ã‚’åˆ†æã—ã¦å­¦ç¿’ã™ã¹ãã‹åˆ¤å®š (LLMä½¿ç”¨)
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            bot_response: Botã®è¿”ç­”ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆåˆ†æç”¨ï¼‰â€»æŒ‡å®šãŒãªã„å ´åˆã¯self.genai_clientã‚’ä½¿ç”¨
        
        Returns:
            åˆ†æçµæœ
        """
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç¢ºä¿
        client = llm_client or self.genai_client
        if not client:
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãªã„å ´åˆã¯ç°¡æ˜“åˆ¤å®š
            info_keywords = ['ã¯', 'ã«ã‚ã‚‹', 'ã§ã™', 'ã ã‚ˆ', 'ã ã‹ã‚‰', 'ãƒ«ãƒ¼ãƒ«']
            has_info = any(kw in user_message for kw in info_keywords)
            return {
                "should_learn": has_info,
                "score": 5 if has_info else 0,
                "type": "knowledge" if has_info else None,
                "content": user_message if has_info else None,
                "keywords": []
            }
        
        # LLMã«ã‚ˆã‚‹åˆ†æ (ai_vs_ai_unified.py ã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)
        prompt = f"""ä»¥ä¸‹ã®ä¼šè©±ã‚’åˆ†æã—ã€AIãŒè¦šãˆã¦ãŠãã¹ãæƒ…å ±ã‚„ãƒ«ãƒ¼ãƒ«ãŒã‚ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã€AIã®ç™ºè¨€ã€‘
{bot_response if bot_response else "(ãªã—)"}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã€‘
{user_message}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›:
{{
  "score": 0-10,
  "rule": "è¡Œå‹•ãƒ«ãƒ¼ãƒ«ï¼ˆ20å­—ä»¥å†…ï¼‰ã¾ãŸã¯null",
  "rule_kw": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"],
  "info": "è¦šãˆã¦ãŠãã¹ãæƒ…å ±ï¼ˆå ´æ‰€ã€æ‰‹é †ã€ãƒ«ãƒ¼ãƒ«ç­‰ï¼‰ã¾ãŸã¯null",
  "info_kw": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"],
  "relations": [
    {{"subject": "ç”°ä¸­", "predicate": "member_of", "object": "é–‹ç™ºãƒãƒ¼ãƒ "}}
  ],
  "questions": ["ã“ã®æƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®æƒ³å®šè³ªå•1", "æƒ³å®šè³ªå•2"],
  "shared": true/false
}}

scoreåˆ¤å®šåŸºæº–ï¼ˆé‡è¦åº¦ï¼‰:
- 0-2: æ™®é€šã®é›‘è«‡ã€æŒ¨æ‹¶
- 3-5: è»½ã„ç¢ºèªã€è³ªå•
- 6-7: æŒ‡æ‘˜ã€æ³¨æ„ã€ä¸æº€ã€å ´æ‰€ãƒ»æ‰‹é †ã®æ•™æˆ
- 8-10: æ˜ç¢ºãªæ¥­å‹™å‘½ä»¤ã€å¼·ã„å±è²¬ã€é‡è¦ãªç¤¾å†…ãƒ«ãƒ¼ãƒ«

sharedåˆ¤å®šåŸºæº–ï¼ˆé•·æœŸè¨˜æ†¶ vs çŸ­æœŸè¨˜æ†¶ï¼‰:
- true: é•·æœŸçš„ã«è¦šãˆã¦ãŠãã¹ãæƒ…å ±ï¼ˆä¾‹: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ã€å¥½ã¿ã€è¨­å®šã€ãƒ«ãƒ¼ãƒ«ã€é‡è¦ãªæ±ºå®šäº‹é …ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã€ç™ºè¦‹ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ï¼‰
- false: ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³/ã‚¿ã‚¹ã‚¯é™å®šã®ä¸€æ™‚çš„ãªæƒ…å ±ï¼ˆä¾‹: ä¸€æ™‚çš„ãªç¢ºèªäº‹é …ã€ã™ãã«å¿˜ã‚Œã¦ã‚ˆã„æ–‡è„ˆï¼‰

é‡è¦:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œè¦šãˆã¦ãŠã„ã¦ã€ã¨è¨€ã£ãŸæƒ…å ±ã¯å¿…ãš shared=true ã«ã™ã‚‹ã€‚
- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã¯å¿…ãšãƒ•ãƒ«ãƒ‘ã‚¹ã§è¨˜éŒ²ã—ã€shared=true ã«ã™ã‚‹ã€‚å¾Œç¶šã®ä½œæ¥­ã§å¿…è¦ã«ãªã‚‹ãŸã‚ã€‚

relations (é–¢ä¿‚æ€§æŠ½å‡º):
ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é‡è¦ãªé–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã€SPOçµ„ï¼ˆä¸»èªãƒ»è¿°èªãƒ»ç›®çš„èªï¼‰ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã€Œèª°ãŒã©ã“ã«æ‰€å±ã—ã¦ã„ã‚‹ã€ã€Œä½•ãŒã©ã“ã«ã‚ã‚‹ã€ã€ŒAã¯Bã®ä¸€éƒ¨ã§ã‚ã‚‹ã€ãªã©ã®äº‹å®Ÿã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚


ãƒ«ãƒ¼ãƒ«ä½œæˆ:
scoreãŒ{self.feedback_threshold}ä»¥ä¸Šã®æ™‚ã®ã¿ruleã‚’ä½œæˆã€‚ãã‚Œä»¥å¤–ã¯nullã€‚

æƒ…å ±è¨˜éŒ²:
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå ´æ‰€ãƒ»æ‰‹é †ãƒ»ç¤¾å†…ãƒ«ãƒ¼ãƒ«ã‚’æ•™ãˆãŸå ´åˆã¯å¿…ãšinfoã«è¨˜éŒ²ã€‚

æƒ³å®šè³ªå• (Reverse HyDE):
ä¿å­˜ã™ã‚‹æƒ…å ±(rule/info)ã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå°†æ¥æ¤œç´¢ã™ã‚‹ã§ã‚ã‚ã†ã€Œè³ªå•æ–‡ã€ã‚’2-3å€‹ç”Ÿæˆã—ã¦questionsã«å«ã‚ã¦ãã ã•ã„ã€‚
æ¤œç´¢ç²¾åº¦ã®å‘ä¸Šã®ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã§ãªãè‡ªç„¶è¨€èªã®è³ªå•å½¢å¼ã‚‚äºˆæ¸¬ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"""

        try:
            # Gemini (google-genai) ã®å ´åˆ
            if hasattr(client, "models"):
                from google.genai import types
                r = client.models.generate_content(
                    model="models/gemini-3-flash-preview",
                    config=types.GenerateContentConfig(response_mime_type="application/json"),
                    contents=[{"role":"user", "parts":[{"text":prompt}]}]
                )
                # .text ã‚’ä½¿ã†ã¨è­¦å‘ŠãŒå‡ºã‚‹ãŸã‚ã€ç›´æ¥ parts ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                response_text = ""
                if r.candidates and r.candidates[0].content and r.candidates[0].content.parts:
                    for part in r.candidates[0].content.parts:
                        if hasattr(part, 'text') and part.text:
                            response_text += part.text
                result = SmartJSONParser.parse(response_text, default={})
            
            # OpenAI ã®å ´åˆ
            elif hasattr(client, "chat"):
                r = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"}
                )
                result = SmartJSONParser.parse(r.choices[0].message.content, default={})
            
            else:
                raise ValueError("Unsupported LLM client")

            # result ãŒãƒªã‚¹ãƒˆã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨ã€è¾æ›¸ã§ãªã„å ´åˆã¯ç©ºè¾æ›¸
            if isinstance(result, list):
                result = result[0] if result else {}
            if not isinstance(result, dict):
                result = {}

            score = result.get("score", 0)
            rule = result.get("rule")
            rule_kw = result.get("rule_kw", [])
            info = result.get("info")
            info_kw = result.get("info_kw", [])
            questions = result.get("questions", [])
            shared = result.get("shared", False)
            relations = result.get("relations", [])
            
            # çµæœã‚’çµ±åˆ
            if info:
                return {
                    "should_learn": True,
                    "score": score,
                    "type": "knowledge",
                    "content": info,
                    "keywords": info_kw,
                    "questions": questions,
                    "shared": shared,
                    "relations": relations
                }
            elif rule and score >= self.feedback_threshold:
                return {
                    "should_learn": True,
                    "score": score,
                    "type": "behavior",
                    "content": rule,
                    "keywords": rule_kw,
                    "questions": questions,
                    "shared": shared,
                    "relations": relations
                }
            else:
                return {
                    "should_learn": False,
                    "score": score,
                    "type": None,
                    "content": None,
                    "keywords": [],
                    "questions": [],
                    "shared": False,
                    "relations": []
                }

        except Exception as e:
            print(f"[MemoryService] Analyze error: {e}")
            return {
                "should_learn": False,
                "score": 0,
                "type": None,
                "content": None,
                "keywords": [],
                "questions": [],
                "shared": False,
                "relations": []
            }
    
    def _resolve_conflict(self, content: str, memory_type: str) -> List[int]:
        """çŸ›ç›¾ã™ã‚‹å¤ã„è¨˜æ†¶ã‚’ç‰¹å®šã™ã‚‹"""
        if not self.genai_client or memory_type == "action_log":
            return []
            
        # é–¢é€£è¨˜æ†¶ã‚’æ¤œç´¢
        related_memories = self.recall(content, top_k=5)
        if not related_memories:
            return []
            
        # å€™è£œã‚’ãƒªã‚¹ãƒˆåŒ–
        candidates = []


        for m in related_memories:
            # ã‚¹ã‚³ã‚¢ãŒä½ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
            if m['score'] < 0.25:
                continue
            candidates.append(f"ID:{m['id']} Content:{m['content']}")
            
        if not candidates:
            return []
            
        candidates_text = "\n".join(candidates)
        
        prompt = f"""æ–°ã—ã„è¨˜æ†¶ã‚’ä¿å­˜ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚æ—¢å­˜ã®è¨˜æ†¶ã¨çŸ›ç›¾ã—ã€ç½®æ›ï¼ˆå‰Šé™¤ï¼‰ã™ã¹ãå¤ã„è¨˜æ†¶ãŒã‚ã‚Œã°IDã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

ã€æ–°ã—ã„è¨˜æ†¶ã€‘
{content}

ã€æ—¢å­˜ã®è¨˜æ†¶å€™è£œã€‘
{candidates_text}

ä»¥ä¸‹ã®æ¡ä»¶ã«å½“ã¦ã¯ã¾ã‚‹å ´åˆã®ã¿ã€ãã®IDã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
1. æ–°ã—ã„è¨˜æ†¶ãŒã€æ—¢å­˜ã®è¨˜æ†¶ã®å†…å®¹ã‚’**æ˜ç¢ºã«å¦å®šãƒ»æ›´æ–°**ã—ã¦ã„ã‚‹ï¼ˆä¾‹:ã€ŒAã¯Bã ã€â†’ã€Œã„ã‚„ã€Aã¯Cã ã€ï¼‰ã€‚
2. å˜ãªã‚‹è¿½åŠ æƒ…å ±ã®å ´åˆã¯å‰Šé™¤ã—ãªã„ã“ã¨ï¼ˆä¾‹:ã€ŒAã¯Bã ã€â†’ã€ŒAã¯Dã§ã‚‚ã‚ã‚‹ã€ï¼‰ã€‚

å‡ºåŠ›å½¢å¼(JSON):
{{
  "delete_ids": [ID1, ID2]
}}
å‰Šé™¤ã™ã¹ãã‚‚ã®ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ [] ã‚’è¿”ã™ã€‚"""

        try:
            if hasattr(self.genai_client, "models"):
                from google.genai import types
                r = self.genai_client.models.generate_content(
                    model="models/gemini-3-flash-preview",
                    config=types.GenerateContentConfig(response_mime_type="application/json"),
                    contents=[{"role":"user", "parts":[{"text":prompt}]}]
                )
                # .text ã‚’ä½¿ã†ã¨è­¦å‘ŠãŒå‡ºã‚‹ãŸã‚ã€ç›´æ¥ parts ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                response_text = ""
                if r.candidates and r.candidates[0].content and r.candidates[0].content.parts:
                    for part in r.candidates[0].content.parts:
                        if hasattr(part, 'text') and part.text:
                            response_text += part.text
                result = SmartJSONParser.parse(response_text, default={})
                return result.get("delete_ids", [])
            else:
                # OpenAI fallback (omitted for brevity, assuming Gemini)
                return []
        except Exception as e:
            print(f"[MemoryService] Conflict resolution failed: {e}")
            return []

    def learn(
        self,
        content: str,
        memory_type: str = "knowledge",
        keywords: Optional[List[str]] = None,
        source: Optional[str] = None,
        run_id: Optional[str] = None,
        check_conflict: bool = True,
        shared: bool = False,
        questions: Optional[List[str]] = None,
        relations: Optional[List[Dict]] = None
    ) -> Dict:
        """
        è¨˜æ†¶ã‚’ä¿å­˜
        
        Args:
            content: è¨˜æ†¶ã™ã‚‹å†…å®¹
            memory_type: "knowledge" or "behavior"
            keywords: æ¤œç´¢ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            source: è¨˜æ†¶ã®å‡ºå…¸
            check_conflict: çŸ›ç›¾ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã‹ (Default: True)
            shared: Trueãªã‚‰å…¨ãƒãƒ£ãƒ³ãƒãƒ«å…±æœ‰(GLOBAL)ã¨ã—ã¦ä¿å­˜ (Default: False)
            questions: æƒ³å®šè³ªå•ãƒªã‚¹ãƒˆ (Reverse HyDEç”¨)
            relations: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ã®é–¢ä¿‚æ€§ãƒªã‚¹ãƒˆ
        """
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ (å®Œå…¨ä¸€è‡´ã«è¿‘ã„ã‚‚ã®)
        # NOTE: For task-level summaries and action logs we want to keep multiple runs.
        if memory_type not in ["action_log", "task_run_summary", "task_run_event"]:
            is_dup, existing = self._is_duplicate(content)
            if is_dup:
                return {
                    "success": False,
                    "is_duplicate": True,
                    "existing": existing
                }
        
        # çŸ›ç›¾è§£æ±º (Conflict Resolution)
        deleted_ids = []
        if check_conflict and memory_type in ["knowledge", "behavior"]:
            deleted_ids = self._resolve_conflict(content, memory_type)
            for del_id in deleted_ids:
                print(f"â™»ï¸ Resolving conflict: Deleting old memory ID {del_id}")
                self.delete(del_id)
        
        # Embeddingç”Ÿæˆ (Reverse HyDE: Content + Keywords + Questions)
        kw_text = ", ".join(keywords) if keywords else ""
        qs_text = "\n".join(questions) if questions else ""
        # æ§‹é€ åŒ–ã‚¿ã‚°ä»˜ã
        emb_text = f"[Questions]:\n{qs_text}\n\n[Content]:\n{content}\n\n[Keywords]:\n{kw_text}".strip()
        emb = self._embed(emb_text)
        
        # ä¿å­˜å…ˆãƒãƒ£ãƒ³ãƒãƒ«æ±ºå®š
        target_channel_id = "GLOBAL" if shared else self.channel_id

        # DBã«ä¿å­˜
        conn = self._get_conn()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO memories (channel_id, router_id, worker_id, run_id, content, type, keywords, questions, source, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            target_channel_id,
            self.router_id,
            self.worker_id,
            run_id,
            content,
            memory_type,
            json.dumps(keywords or [], ensure_ascii=False),
            json.dumps(questions or [], ensure_ascii=False),
            source,
            self._serialize_embedding(emb)
        ))
        memory_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        # ã‚°ãƒ©ãƒ•ã«é–¢ä¿‚æ€§ã‚’ä¿å­˜
        if self.graph and relations and memory_id is not None:
            for rel in relations:
                try:
                    self.graph.add_relation(
                        subject=rel["subject"],
                        predicate=rel["predicate"],
                        object=rel["object"],
                        memory_id=memory_id
                    )
                except Exception as e:
                    print(f"âš ï¸ Failed to add relation to graph: {e}")

        return {
            "success": True,
            "is_duplicate": False,
            "memory_id": memory_id,
            "deleted_ids": deleted_ids
        }
    
    def delete(self, memory_id: int) -> bool:
        """è¨˜æ†¶ã‚’å‰Šé™¤"""
        # ã‚°ãƒ©ãƒ•ã‹ã‚‰é–¢ä¿‚æ€§ã‚’å‰Šé™¤
        if self.graph:
            try:
                self.graph.delete_relations(memory_id)
            except Exception as e:
                print(f"âš ï¸ Failed to delete relations from graph: {e}")

        conn = self._get_conn()
        cursor = conn.cursor()
        cursor.execute('DELETE FROM memories WHERE id = ?', (memory_id,))
        deleted = cursor.rowcount > 0
        conn.commit()
        conn.close()
        return deleted
    
    def list_all(self) -> List[Dict]:
        """å…¨è¨˜æ†¶ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        return self._get_all_memories()
    
    def count(self) -> int:
        """è¨˜æ†¶æ•°ã‚’å–å¾—"""
        conn = self._get_conn()
        cursor = conn.cursor()
        if self.channel_id:
            cursor.execute("SELECT COUNT(*) FROM memories WHERE channel_id = ? OR channel_id = 'GLOBAL'", (self.channel_id,))
        else:
            cursor.execute('SELECT COUNT(*) FROM memories')
        count = cursor.fetchone()[0]
        conn.close()
        return count
    
    def get_context_prompt(self, query: str) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹è¨˜æ†¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹æ–‡å­—åˆ—
        """
        memories = self.recall(query)
        if not memories:
            return ""
        
        lines = ["ã€è¨˜æ†¶ã—ã¦ã„ã‚‹æƒ…å ±ã€‘"]
        for m in memories:
            lines.append(f"- {m['content']}")
        
        return "\n".join(lines)

    def record_action_result(self, action: str, params: Dict, result: str, success: bool, feedback: int = 0):
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œçµæœã‚’è¨˜éŒ²ã™ã‚‹ (Agent Runtimeã‹ã‚‰å‘¼ã°ã‚Œã‚‹)
        
        NOTE: å¤±æ•—ã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿è¨˜éŒ²ã™ã‚‹ï¼ˆæˆåŠŸã¯å†—é•·ãªãŸã‚ï¼‰
        - å¤±æ•—ã‚’è¨˜éŒ²ã™ã‚‹ã“ã¨ã§ã€åŒã˜å¤±æ•—ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã‚ˆã†å­¦ç¿’ã«æ´»ç”¨
        - æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ task_run_summary ã§ã‚«ãƒãƒ¼
        """
        # Success handling:
        # We don't store success action logs (to reduce noise), but we DO use success as a signal
        # that older failures for the same tool may have become stale (system updates, auth fixes, etc.).
        # To prevent stale failures from being recalled, we proactively delete old failure memories
        # for the same action in the same channel scope.
        if success:
            try:
                deleted = self._cleanup_action_log_failures(action=action, max_delete=50)
                if deleted:
                    print(f"ğŸ§  [Memory] Cleaned up {deleted} stale action_log failures for action={action}")
            except Exception:
                pass
            return
        
        try:
            content = f"Action: {action}, Params: {json.dumps(params, ensure_ascii=False)}, Success: {success}, Result: {result[:200]}"
            self.learn(
                content=content,
                memory_type="action_log",
                keywords=[action, "failure", "error"],
                source="system_runtime"
            )
            print(f"ğŸ§  [Memory] Recorded FAILED Action: {action}")
        except Exception as e:
            print(f"âš ï¸ Failed to record action result: {e}")

    def _cleanup_action_log_failures(self, action: str, max_delete: int = 50) -> int:
        """
        Delete older failure memories for a given action (tool name).
        This keeps action_log from poisoning future runs after the system/tool behavior changes.

        Scope:
        - Only deletes type='action_log'
        - Only within this MemoryService scope (channel_id); GLOBAL is not targeted.
        - Matches by content prefix "Action: {action},"
        """
        if not action:
            return 0
        if not self.channel_id:
            # Without a channel scope, be conservative and do not delete.
            return 0

        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            cursor.execute(
                """
                SELECT id
                FROM memories
                WHERE channel_id = ?
                  AND type = 'action_log'
                  AND content LIKE ?
                ORDER BY id DESC
                LIMIT ?
                """,
                (self.channel_id, f"Action: {action},%", int(max_delete)),
            )
            rows = cursor.fetchall()
            ids = [r[0] for r in rows if r and r[0] is not None]
            if not ids:
                return 0
            cursor.executemany("DELETE FROM memories WHERE id = ?", [(i,) for i in ids])
            deleted = cursor.rowcount if cursor.rowcount is not None else len(ids)
            conn.commit()
            return int(deleted)
        finally:
            conn.close()

    # ------------------------------------------------------------------
    # Task Run Correlation APIs
    # ------------------------------------------------------------------
    def record_task_run_event(
        self,
        run_id: str,
        tool_name: str,
        params: Dict[str, Any],
        result: Dict[str, Any],
        success: bool,
        error_type: Optional[str] = None,
        recovery_hint: Optional[str] = None,
        channel_id: Optional[str] = None,
        router_id: Optional[str] = None,
        skill_id: Optional[str] = None,
    ) -> None:
        """
        Store a tool execution event tied to a specific task_run_id.
        This is used to build TaskRunSummary across multiple workers.
        """
        if not run_id:
            return
        try:
            conn = self._get_conn()
            cursor = conn.cursor()
            params_json = json.dumps(params or {}, ensure_ascii=False)
            # Keep result preview compact (avoid secrets / huge blobs)
            result_preview = ""
            try:
                result_preview = json.dumps(result, ensure_ascii=False)[:2000]
            except Exception:
                result_preview = str(result)[:2000]
            cursor.execute(
                """
                INSERT INTO task_run_events
                (run_id, channel_id, router_id, skill_id, tool_name, params_json, result_preview,
                 success, error_type, recovery_hint)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    str(run_id),
                    channel_id or self.channel_id,
                    router_id,
                    skill_id,
                    tool_name,
                    params_json,
                    result_preview,
                    1 if success else 0,
                    error_type,
                    recovery_hint,
                ),
            )
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"âš ï¸ Failed to record task run event: {e}")

    def list_task_run_events(self, run_id: str, channel_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all tool events for a given run_id (ordered)."""
        if not run_id:
            return []
        try:
            conn = self._get_conn()
            cursor = conn.cursor()
            
            # Use provided channel_id or the service's default
            target_channel = channel_id or self.channel_id
            
            query = """
                SELECT tool_name, params_json, result_preview, success, error_type, recovery_hint, skill_id, router_id, created_at
                FROM task_run_events
                WHERE run_id = ?
            """
            params = [str(run_id)]
            
            if target_channel:
                query += " AND channel_id = ?"
                params.append(target_channel)
                
            query += " ORDER BY id ASC"
            
            cursor.execute(query, tuple(params))
            rows = cursor.fetchall()
            conn.close()
            out = []
            for r in rows:
                out.append(
                    {
                        "tool_name": r[0],
                        "params_json": r[1],
                        "result_preview": r[2],
                        "success": bool(r[3]),
                        "error_type": r[4],
                        "recovery_hint": r[5],
                        "skill_id": r[6],
                        "router_id": r[7],
                        "created_at": r[8],
                    }
                )
            return out
        except Exception as e:
            print(f"âš ï¸ Failed to list task run events: {e}")
            return []
